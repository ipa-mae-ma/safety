[2018-09-05 09:05:42.269156 UTC] Starting env pool
[2018-09-05 09:05:42.329960 UTC] Starting iteration 0
[2018-09-05 09:05:42.330568 UTC] Start collecting samples
[2018-09-05 09:05:42.680428 UTC] Computing input variables for policy optimization
[2018-09-05 09:05:42.766498 UTC] Computing policy gradient
[2018-09-05 09:05:42.784709 UTC] Updating baseline
[2018-09-05 09:05:42.947009 UTC] Computing logging information
-------------------------------------
| Iteration            | 0          |
| SurrLoss             | -0.0026496 |
| Entropy              | 0.6925     |
| Perplexity           | 1.9987     |
| AveragePolicyProb[0] | 0.50155    |
| AveragePolicyProb[1] | 0.49845    |
| AverageReturn        | 23.462     |
| MinReturn            | 9          |
| MaxReturn            | 81         |
| StdReturn            | 11.748     |
| AverageEpisodeLength | 23.462     |
| MinEpisodeLength     | 9          |
| MaxEpisodeLength     | 81         |
| StdEpisodeLength     | 11.748     |
| TotalNEpisodes       | 78         |
| TotalNSamples        | 1830       |
| ExplainedVariance    | -0.0058665 |
-------------------------------------
[2018-09-05 09:05:42.974529 UTC] Saving snapshot
[2018-09-05 09:05:42.983019 UTC] Starting iteration 1
[2018-09-05 09:05:42.983175 UTC] Start collecting samples
[2018-09-05 09:05:43.284189 UTC] Computing input variables for policy optimization
[2018-09-05 09:05:43.326373 UTC] Computing policy gradient
[2018-09-05 09:05:43.336666 UTC] Updating baseline
[2018-09-05 09:05:43.470688 UTC] Computing logging information
------------------------------------
| Iteration            | 1         |
| SurrLoss             | -0.028403 |
| Entropy              | 0.63881   |
| Perplexity           | 1.8942    |
| AveragePolicyProb[0] | 0.48601   |
| AveragePolicyProb[1] | 0.51399   |
| AverageReturn        | 30.72     |
| MinReturn            | 9         |
| MaxReturn            | 109       |
| StdReturn            | 18.103    |
| AverageEpisodeLength | 30.72     |
| MinEpisodeLength     | 9         |
| MaxEpisodeLength     | 109       |
| StdEpisodeLength     | 18.103    |
| TotalNEpisodes       | 124       |
| TotalNSamples        | 3619      |
| ExplainedVariance    | 0.15902   |
------------------------------------
[2018-09-05 09:05:43.499219 UTC] Saving snapshot
[2018-09-05 09:05:43.507429 UTC] Starting iteration 2
[2018-09-05 09:05:43.507586 UTC] Start collecting samples
[2018-09-05 09:05:43.774110 UTC] Computing input variables for policy optimization
[2018-09-05 09:05:43.803683 UTC] Computing policy gradient
[2018-09-05 09:05:43.814706 UTC] Updating baseline
[2018-09-05 09:05:43.959437 UTC] Computing logging information
------------------------------------
| Iteration            | 2         |
| SurrLoss             | -0.044707 |
| Entropy              | 0.60104   |
| Perplexity           | 1.824     |
| AveragePolicyProb[0] | 0.48011   |
| AveragePolicyProb[1] | 0.51989   |
| AverageReturn        | 38.42     |
| MinReturn            | 10        |
| MaxReturn            | 112       |
| StdReturn            | 22.32     |
| AverageEpisodeLength | 38.42     |
| MinEpisodeLength     | 10        |
| MaxEpisodeLength     | 112       |
| StdEpisodeLength     | 22.32     |
| TotalNEpisodes       | 148       |
| TotalNSamples        | 5017      |
| ExplainedVariance    | 0.33974   |
------------------------------------
[2018-09-05 09:05:43.987874 UTC] Saving snapshot
[2018-09-05 09:05:43.996656 UTC] Starting iteration 3
[2018-09-05 09:05:43.996804 UTC] Start collecting samples
[2018-09-05 09:05:44.238288 UTC] Computing input variables for policy optimization
[2018-09-05 09:05:44.261399 UTC] Computing policy gradient
[2018-09-05 09:05:44.271086 UTC] Updating baseline
[2018-09-05 09:05:44.409487 UTC] Computing logging information
------------------------------------
| Iteration            | 3         |
| SurrLoss             | -0.022233 |
| Entropy              | 0.56557   |
| Perplexity           | 1.7605    |
| AveragePolicyProb[0] | 0.51612   |
| AveragePolicyProb[1] | 0.48388   |
| AverageReturn        | 53.1      |
| MinReturn            | 10        |
| MaxReturn            | 200       |
| StdReturn            | 42.011    |
| AverageEpisodeLength | 53.1      |
| MinEpisodeLength     | 10        |
| MaxEpisodeLength     | 200       |
| StdEpisodeLength     | 42.011    |
| TotalNEpisodes       | 161       |
| TotalNSamples        | 6783      |
| ExplainedVariance    | 0.33266   |
------------------------------------
[2018-09-05 09:05:44.437183 UTC] Saving snapshot
[2018-09-05 09:05:44.445919 UTC] Starting iteration 4
[2018-09-05 09:05:44.446063 UTC] Start collecting samples
[2018-09-05 09:05:44.705667 UTC] Computing input variables for policy optimization
[2018-09-05 09:05:44.728610 UTC] Computing policy gradient
[2018-09-05 09:05:44.738287 UTC] Updating baseline
[2018-09-05 09:05:44.883506 UTC] Computing logging information
------------------------------------
| Iteration            | 4         |
| SurrLoss             | -0.015231 |
| Entropy              | 0.52242   |
| Perplexity           | 1.6861    |
| AveragePolicyProb[0] | 0.50026   |
| AveragePolicyProb[1] | 0.49974   |
| AverageReturn        | 68.93     |
| MinReturn            | 10        |
| MaxReturn            | 200       |
| StdReturn            | 52.911    |
| AverageEpisodeLength | 68.93     |
| MinEpisodeLength     | 10        |
| MaxEpisodeLength     | 200       |
| StdEpisodeLength     | 52.911    |
| TotalNEpisodes       | 173       |
| TotalNSamples        | 8606      |
| ExplainedVariance    | 0.75874   |
------------------------------------
[2018-09-05 09:05:44.912415 UTC] Saving snapshot
[2018-09-05 09:05:44.920481 UTC] Starting iteration 5
[2018-09-05 09:05:44.920643 UTC] Start collecting samples
[2018-09-05 09:05:45.169883 UTC] Computing input variables for policy optimization
[2018-09-05 09:05:45.191443 UTC] Computing policy gradient
[2018-09-05 09:05:45.200325 UTC] Updating baseline
[2018-09-05 09:05:45.330542 UTC] Computing logging information
------------------------------------
| Iteration            | 5         |
| SurrLoss             | -0.014955 |
| Entropy              | 0.48208   |
| Perplexity           | 1.6194    |
| AveragePolicyProb[0] | 0.49306   |
| AveragePolicyProb[1] | 0.50694   |
| AverageReturn        | 85.61     |
| MinReturn            | 16        |
| MaxReturn            | 200       |
| StdReturn            | 59.692    |
| AverageEpisodeLength | 85.61     |
| MinEpisodeLength     | 16        |
| MaxEpisodeLength     | 200       |
| StdEpisodeLength     | 59.692    |
| TotalNEpisodes       | 184       |
| TotalNSamples        | 10523     |
| ExplainedVariance    | 0.70409   |
------------------------------------
[2018-09-05 09:05:45.358914 UTC] Saving snapshot
[2018-09-05 09:05:45.367373 UTC] Starting iteration 6
[2018-09-05 09:05:45.367532 UTC] Start collecting samples
[2018-09-05 09:05:45.640606 UTC] Computing input variables for policy optimization
[2018-09-05 09:05:45.663902 UTC] Computing policy gradient
[2018-09-05 09:05:45.673257 UTC] Updating baseline
[2018-09-05 09:05:45.795034 UTC] Computing logging information
------------------------------------
| Iteration            | 6         |
| SurrLoss             | -0.020996 |
| Entropy              | 0.44849   |
| Perplexity           | 1.5659    |
| AveragePolicyProb[0] | 0.4747    |
| AveragePolicyProb[1] | 0.5253    |
| AverageReturn        | 104.33    |
| MinReturn            | 18        |
| MaxReturn            | 200       |
| StdReturn            | 62.285    |
| AverageEpisodeLength | 104.33    |
| MinEpisodeLength     | 18        |
| MaxEpisodeLength     | 200       |
| StdEpisodeLength     | 62.285    |
| TotalNEpisodes       | 199       |
| TotalNSamples        | 12869     |
| ExplainedVariance    | 0.62656   |
------------------------------------
[2018-09-05 09:05:45.823713 UTC] Saving snapshot
[2018-09-05 09:05:45.832069 UTC] Starting iteration 7
[2018-09-05 09:05:45.832241 UTC] Start collecting samples
[2018-09-05 09:05:46.074581 UTC] Computing input variables for policy optimization
[2018-09-05 09:05:46.094996 UTC] Computing policy gradient
[2018-09-05 09:05:46.103870 UTC] Updating baseline
[2018-09-05 09:05:46.223542 UTC] Computing logging information
------------------------------------
| Iteration            | 7         |
| SurrLoss             | -0.010917 |
| Entropy              | 0.43477   |
| Perplexity           | 1.5446    |
| AveragePolicyProb[0] | 0.47976   |
| AveragePolicyProb[1] | 0.52024   |
| AverageReturn        | 116.1     |
| MinReturn            | 18        |
| MaxReturn            | 200       |
| StdReturn            | 62.136    |
| AverageEpisodeLength | 116.1     |
| MinEpisodeLength     | 18        |
| MaxEpisodeLength     | 200       |
| StdEpisodeLength     | 62.136    |
| TotalNEpisodes       | 208       |
| TotalNSamples        | 14413     |
| ExplainedVariance    | 0.78968   |
------------------------------------
[2018-09-05 09:05:46.253348 UTC] Saving snapshot
[2018-09-05 09:05:46.261979 UTC] Starting iteration 8
[2018-09-05 09:05:46.262144 UTC] Start collecting samples
[2018-09-05 09:05:46.503685 UTC] Computing input variables for policy optimization
[2018-09-05 09:05:46.525614 UTC] Computing policy gradient
[2018-09-05 09:05:46.535309 UTC] Updating baseline
[2018-09-05 09:05:46.679078 UTC] Computing logging information
------------------------------------
| Iteration            | 8         |
| SurrLoss             | 0.0025737 |
| Entropy              | 0.40669   |
| Perplexity           | 1.5018    |
| AveragePolicyProb[0] | 0.48865   |
| AveragePolicyProb[1] | 0.51135   |
| AverageReturn        | 130.44    |
| MinReturn            | 29        |
| MaxReturn            | 200       |
| StdReturn            | 59.865    |
| AverageEpisodeLength | 130.44    |
| MinEpisodeLength     | 29        |
| MaxEpisodeLength     | 200       |
| StdEpisodeLength     | 59.865    |
| TotalNEpisodes       | 219       |
| TotalNSamples        | 16363     |
| ExplainedVariance    | 0.57212   |
------------------------------------
[2018-09-05 09:05:46.708358 UTC] Saving snapshot
[2018-09-05 09:05:46.716489 UTC] Starting iteration 9
[2018-09-05 09:05:46.716647 UTC] Start collecting samples
[2018-09-05 09:05:46.979251 UTC] Computing input variables for policy optimization
[2018-09-05 09:05:47.001459 UTC] Computing policy gradient
[2018-09-05 09:05:47.010628 UTC] Updating baseline
[2018-09-05 09:05:47.146289 UTC] Computing logging information
-----------------------------------
| Iteration            | 9        |
| SurrLoss             | 0.007705 |
| Entropy              | 0.37855  |
| Perplexity           | 1.4602   |
| AveragePolicyProb[0] | 0.51787  |
| AveragePolicyProb[1] | 0.48213  |
| AverageReturn        | 148.51   |
| MinReturn            | 29       |
| MaxReturn            | 200      |
| StdReturn            | 54.979   |
| AverageEpisodeLength | 148.51   |
| MinEpisodeLength     | 29       |
| MaxEpisodeLength     | 200      |
| StdEpisodeLength     | 54.979   |
| TotalNEpisodes       | 232      |
| TotalNSamples        | 18888    |
| ExplainedVariance    | 0.52047  |
-----------------------------------
[2018-09-05 09:05:47.174996 UTC] Saving snapshot
[2018-09-05 09:05:47.183509 UTC] Starting iteration 10
[2018-09-05 09:05:47.183659 UTC] Start collecting samples
[2018-09-05 09:05:47.443301 UTC] Computing input variables for policy optimization
[2018-09-05 09:05:47.468458 UTC] Computing policy gradient
[2018-09-05 09:05:47.478282 UTC] Updating baseline
[2018-09-05 09:05:47.609421 UTC] Computing logging information
-------------------------------------
| Iteration            | 10         |
| SurrLoss             | -0.0013558 |
| Entropy              | 0.35773    |
| Perplexity           | 1.4301     |
| AveragePolicyProb[0] | 0.54669    |
| AveragePolicyProb[1] | 0.45331    |
| AverageReturn        | 159.52     |
| MinReturn            | 33         |
| MaxReturn            | 200        |
| StdReturn            | 45.105     |
| AverageEpisodeLength | 159.52     |
| MinEpisodeLength     | 33         |
| MaxEpisodeLength     | 200        |
| StdEpisodeLength     | 45.105     |
| TotalNEpisodes       | 245        |
| TotalNSamples        | 20828      |
| ExplainedVariance    | 0.80884    |
-------------------------------------
[2018-09-05 09:05:47.640274 UTC] Saving snapshot
[2018-09-05 09:05:47.648867 UTC] Starting iteration 11
[2018-09-05 09:05:47.649015 UTC] Start collecting samples
[2018-09-05 09:05:47.909581 UTC] Computing input variables for policy optimization
[2018-09-05 09:05:47.931661 UTC] Computing policy gradient
[2018-09-05 09:05:47.940984 UTC] Updating baseline
[2018-09-05 09:05:48.061158 UTC] Computing logging information
------------------------------------
| Iteration            | 11        |
| SurrLoss             | -0.030702 |
| Entropy              | 0.34997   |
| Perplexity           | 1.419     |
| AveragePolicyProb[0] | 0.54475   |
| AveragePolicyProb[1] | 0.45525   |
| AverageReturn        | 164.01    |
| MinReturn            | 64        |
| MaxReturn            | 200       |
| StdReturn            | 37.866    |
| AverageEpisodeLength | 164.01    |
| MinEpisodeLength     | 64        |
| MaxEpisodeLength     | 200       |
| StdEpisodeLength     | 37.866    |
| TotalNEpisodes       | 258       |
| TotalNSamples        | 22628     |
| ExplainedVariance    | 0.93394   |
------------------------------------
[2018-09-05 09:05:48.090798 UTC] Saving snapshot
[2018-09-05 09:05:48.098822 UTC] Starting iteration 12
[2018-09-05 09:05:48.098963 UTC] Start collecting samples
[2018-09-05 09:05:48.351764 UTC] Computing input variables for policy optimization
[2018-09-05 09:05:48.376336 UTC] Computing policy gradient
[2018-09-05 09:05:48.387611 UTC] Updating baseline
[2018-09-05 09:05:48.520900 UTC] Computing logging information
------------------------------------
| Iteration            | 12        |
| SurrLoss             | -0.017465 |
| Entropy              | 0.33331   |
| Perplexity           | 1.3956    |
| AveragePolicyProb[0] | 0.53551   |
| AveragePolicyProb[1] | 0.46449   |
| AverageReturn        | 163.52    |
| MinReturn            | 84        |
| MaxReturn            | 200       |
| StdReturn            | 35.157    |
| AverageEpisodeLength | 163.52    |
| MinEpisodeLength     | 84        |
| MaxEpisodeLength     | 200       |
| StdEpisodeLength     | 35.157    |
| TotalNEpisodes       | 273       |
| TotalNSamples        | 24958     |
| ExplainedVariance    | 0.89459   |
------------------------------------
[2018-09-05 09:05:48.552538 UTC] Saving snapshot
[2018-09-05 09:05:48.560774 UTC] Starting iteration 13
[2018-09-05 09:05:48.560952 UTC] Start collecting samples
[2018-09-05 09:05:48.832210 UTC] Computing input variables for policy optimization
[2018-09-05 09:05:48.854097 UTC] Computing policy gradient
[2018-09-05 09:05:48.864087 UTC] Updating baseline
[2018-09-05 09:05:48.993398 UTC] Computing logging information
-----------------------------------
| Iteration            | 13       |
| SurrLoss             | -0.02952 |
| Entropy              | 0.32417  |
| Perplexity           | 1.3829   |
| AveragePolicyProb[0] | 0.52268  |
| AveragePolicyProb[1] | 0.47732  |
| AverageReturn        | 163.34   |
| MinReturn            | 84       |
| MaxReturn            | 200      |
| StdReturn            | 35.182   |
| AverageEpisodeLength | 163.34   |
| MinEpisodeLength     | 84       |
| MaxEpisodeLength     | 200      |
| StdEpisodeLength     | 35.182   |
| TotalNEpisodes       | 282      |
| TotalNSamples        | 26570    |
| ExplainedVariance    | 0.90237  |
-----------------------------------
[2018-09-05 09:05:49.025039 UTC] Saving snapshot
[2018-09-05 09:05:49.033496 UTC] Starting iteration 14
[2018-09-05 09:05:49.033647 UTC] Start collecting samples
[2018-09-05 09:05:49.290649 UTC] Computing input variables for policy optimization
[2018-09-05 09:05:49.311688 UTC] Computing policy gradient
[2018-09-05 09:05:49.320511 UTC] Updating baseline
[2018-09-05 09:05:49.441095 UTC] Computing logging information
-------------------------------------
| Iteration            | 14         |
| SurrLoss             | -0.0048555 |
| Entropy              | 0.31573    |
| Perplexity           | 1.3713     |
| AveragePolicyProb[0] | 0.52219    |
| AveragePolicyProb[1] | 0.47781    |
| AverageReturn        | 167.56     |
| MinReturn            | 96         |
| MaxReturn            | 200        |
| StdReturn            | 32.786     |
| AverageEpisodeLength | 167.56     |
| MinEpisodeLength     | 96         |
| MaxEpisodeLength     | 200        |
| StdEpisodeLength     | 32.786     |
| TotalNEpisodes       | 292        |
| TotalNSamples        | 28478      |
| ExplainedVariance    | 0.63103    |
-------------------------------------
[2018-09-05 09:05:49.472902 UTC] Saving snapshot
[2018-09-05 09:05:49.480804 UTC] Starting iteration 15
[2018-09-05 09:05:49.480944 UTC] Start collecting samples
[2018-09-05 09:05:49.736821 UTC] Computing input variables for policy optimization
[2018-09-05 09:05:49.758774 UTC] Computing policy gradient
[2018-09-05 09:05:49.768469 UTC] Updating baseline
[2018-09-05 09:05:49.895788 UTC] Computing logging information
------------------------------------
| Iteration            | 15        |
| SurrLoss             | -0.011926 |
| Entropy              | 0.31158   |
| Perplexity           | 1.3656    |
| AveragePolicyProb[0] | 0.50591   |
| AveragePolicyProb[1] | 0.49409   |
| AverageReturn        | 170.81    |
| MinReturn            | 96        |
| MaxReturn            | 200       |
| StdReturn            | 31.68     |
| AverageEpisodeLength | 170.81    |
| MinEpisodeLength     | 96        |
| MaxEpisodeLength     | 200       |
| StdEpisodeLength     | 31.68     |
| TotalNEpisodes       | 303       |
| TotalNSamples        | 30664     |
| ExplainedVariance    | 0.57054   |
------------------------------------
[2018-09-05 09:05:49.926257 UTC] Saving snapshot
[2018-09-05 09:05:49.934765 UTC] Starting iteration 16
[2018-09-05 09:05:49.934920 UTC] Start collecting samples
[2018-09-05 09:05:50.228697 UTC] Computing input variables for policy optimization
[2018-09-05 09:05:50.248178 UTC] Computing policy gradient
[2018-09-05 09:05:50.258941 UTC] Updating baseline
[2018-09-05 09:05:50.401089 UTC] Computing logging information
------------------------------------
| Iteration            | 16        |
| SurrLoss             | 0.0083412 |
| Entropy              | 0.3282    |
| Perplexity           | 1.3885    |
| AveragePolicyProb[0] | 0.48459   |
| AveragePolicyProb[1] | 0.51541   |
| AverageReturn        | 172.51    |
| MinReturn            | 96        |
| MaxReturn            | 200       |
| StdReturn            | 31.072    |
| AverageEpisodeLength | 172.51    |
| MinEpisodeLength     | 96        |
| MaxEpisodeLength     | 200       |
| StdEpisodeLength     | 31.072    |
| TotalNEpisodes       | 310       |
| TotalNSamples        | 32064     |
| ExplainedVariance    | 0.20447   |
------------------------------------
[2018-09-05 09:05:50.432656 UTC] Saving snapshot
[2018-09-05 09:05:50.440546 UTC] Starting iteration 17
[2018-09-05 09:05:50.440685 UTC] Start collecting samples
[2018-09-05 09:05:50.699503 UTC] Computing input variables for policy optimization
[2018-09-05 09:05:50.722732 UTC] Computing policy gradient
[2018-09-05 09:05:50.732272 UTC] Updating baseline
[2018-09-05 09:05:50.856679 UTC] Computing logging information
------------------------------------
| Iteration            | 17        |
| SurrLoss             | -0.026537 |
| Entropy              | 0.31833   |
| Perplexity           | 1.3748    |
| AveragePolicyProb[0] | 0.48881   |
| AveragePolicyProb[1] | 0.51119   |
| AverageReturn        | 175.01    |
| MinReturn            | 96        |
| MaxReturn            | 200       |
| StdReturn            | 30.578    |
| AverageEpisodeLength | 175.01    |
| MinEpisodeLength     | 96        |
| MaxEpisodeLength     | 200       |
| StdEpisodeLength     | 30.578    |
| TotalNEpisodes       | 323       |
| TotalNSamples        | 34664     |
| ExplainedVariance    | -0.46647  |
------------------------------------
[2018-09-05 09:05:50.887582 UTC] Saving snapshot
[2018-09-05 09:05:50.895545 UTC] Starting iteration 18
[2018-09-05 09:05:50.895694 UTC] Start collecting samples
[2018-09-05 09:05:51.162576 UTC] Computing input variables for policy optimization
[2018-09-05 09:05:51.181476 UTC] Computing policy gradient
[2018-09-05 09:05:51.190209 UTC] Updating baseline
[2018-09-05 09:05:51.322726 UTC] Computing logging information
------------------------------------
| Iteration            | 18        |
| SurrLoss             | -0.004264 |
| Entropy              | 0.31857   |
| Perplexity           | 1.3752    |
| AveragePolicyProb[0] | 0.49225   |
| AveragePolicyProb[1] | 0.50775   |
| AverageReturn        | 175.76    |
| MinReturn            | 96        |
| MaxReturn            | 200       |
| StdReturn            | 30.913    |
| AverageEpisodeLength | 175.76    |
| MinEpisodeLength     | 96        |
| MaxEpisodeLength     | 200       |
| StdEpisodeLength     | 30.913    |
| TotalNEpisodes       | 331       |
| TotalNSamples        | 36264     |
| ExplainedVariance    | 0.13433   |
------------------------------------
[2018-09-05 09:05:51.353179 UTC] Saving snapshot
[2018-09-05 09:05:51.361171 UTC] Starting iteration 19
[2018-09-05 09:05:51.361317 UTC] Start collecting samples
[2018-09-05 09:05:51.616088 UTC] Computing input variables for policy optimization
[2018-09-05 09:05:51.637192 UTC] Computing policy gradient
[2018-09-05 09:05:51.645919 UTC] Updating baseline
[2018-09-05 09:05:51.776936 UTC] Computing logging information
-------------------------------------
| Iteration            | 19         |
| SurrLoss             | 3.7266e-05 |
| Entropy              | 0.31406    |
| Perplexity           | 1.369      |
| AveragePolicyProb[0] | 0.48807    |
| AveragePolicyProb[1] | 0.51193    |
| AverageReturn        | 179.41     |
| MinReturn            | 96         |
| MaxReturn            | 200        |
| StdReturn            | 29.674     |
| AverageEpisodeLength | 179.41     |
| MinEpisodeLength     | 96         |
| MaxEpisodeLength     | 200        |
| StdEpisodeLength     | 29.674     |
| TotalNEpisodes       | 341        |
| TotalNSamples        | 38264      |
| ExplainedVariance    | 0.36812    |
-------------------------------------
[2018-09-05 09:05:51.818687 UTC] Saving snapshot
[2018-09-05 09:05:51.829661 UTC] Starting iteration 20
[2018-09-05 09:05:51.829836 UTC] Start collecting samples
[2018-09-05 09:05:52.074140 UTC] Computing input variables for policy optimization
[2018-09-05 09:05:52.097297 UTC] Computing policy gradient
[2018-09-05 09:05:52.105888 UTC] Updating baseline
[2018-09-05 09:05:52.226762 UTC] Computing logging information
------------------------------------
| Iteration            | 20        |
| SurrLoss             | -0.016755 |
| Entropy              | 0.30915   |
| Perplexity           | 1.3623    |
| AveragePolicyProb[0] | 0.49305   |
| AveragePolicyProb[1] | 0.50695   |
| AverageReturn        | 186.64    |
| MinReturn            | 96        |
| MaxReturn            | 200       |
| StdReturn            | 24.949    |
| AverageEpisodeLength | 186.64    |
| MinEpisodeLength     | 96        |
| MaxEpisodeLength     | 200       |
| StdEpisodeLength     | 24.949    |
| TotalNEpisodes       | 353       |
| TotalNSamples        | 40664     |
| ExplainedVariance    | 0.596     |
------------------------------------
[2018-09-05 09:05:52.258572 UTC] Saving snapshot
[2018-09-05 09:05:52.267139 UTC] Starting iteration 21
[2018-09-05 09:05:52.267296 UTC] Start collecting samples
[2018-09-05 09:05:52.523846 UTC] Computing input variables for policy optimization
[2018-09-05 09:05:52.544483 UTC] Computing policy gradient
[2018-09-05 09:05:52.553457 UTC] Updating baseline
[2018-09-05 09:05:52.693533 UTC] Computing logging information
-----------------------------------
| Iteration            | 21       |
| SurrLoss             | 0.012449 |
| Entropy              | 0.29635  |
| Perplexity           | 1.3449   |
| AveragePolicyProb[0] | 0.50203  |
| AveragePolicyProb[1] | 0.49797  |
| AverageReturn        | 192.28   |
| MinReturn            | 117      |
| MaxReturn            | 200      |
| StdReturn            | 18.514   |
| AverageEpisodeLength | 192.28   |
| MinEpisodeLength     | 117      |
| MaxEpisodeLength     | 200      |
| StdEpisodeLength     | 18.514   |
| TotalNEpisodes       | 362      |
| TotalNSamples        | 42464    |
| ExplainedVariance    | 0.5032   |
-----------------------------------
[2018-09-05 09:05:52.724839 UTC] Saving snapshot
[2018-09-05 09:05:52.732782 UTC] Starting iteration 22
[2018-09-05 09:05:52.732942 UTC] Start collecting samples
[2018-09-05 09:05:52.995143 UTC] Computing input variables for policy optimization
[2018-09-05 09:05:53.015693 UTC] Computing policy gradient
[2018-09-05 09:05:53.025749 UTC] Updating baseline
[2018-09-05 09:05:53.142604 UTC] Computing logging information
------------------------------------
| Iteration            | 22        |
| SurrLoss             | -0.014381 |
| Entropy              | 0.29566   |
| Perplexity           | 1.344     |
| AveragePolicyProb[0] | 0.49133   |
| AveragePolicyProb[1] | 0.50867   |
| AverageReturn        | 196.71    |
| MinReturn            | 138       |
| MaxReturn            | 200       |
| StdReturn            | 11.114    |
| AverageEpisodeLength | 196.71    |
| MinEpisodeLength     | 138       |
| MaxEpisodeLength     | 200       |
| StdEpisodeLength     | 11.114    |
| TotalNEpisodes       | 372       |
| TotalNSamples        | 44464     |
| ExplainedVariance    | 0.62159   |
------------------------------------
[2018-09-05 09:05:53.174936 UTC] Saving snapshot
[2018-09-05 09:05:53.182975 UTC] Starting iteration 23
[2018-09-05 09:05:53.183128 UTC] Start collecting samples
[2018-09-05 09:05:53.442608 UTC] Computing input variables for policy optimization
[2018-09-05 09:05:53.463564 UTC] Computing policy gradient
[2018-09-05 09:05:53.472521 UTC] Updating baseline
[2018-09-05 09:05:53.617142 UTC] Computing logging information
------------------------------------
| Iteration            | 23        |
| SurrLoss             | 0.0063499 |
| Entropy              | 0.30406   |
| Perplexity           | 1.3553    |
| AveragePolicyProb[0] | 0.5048    |
| AveragePolicyProb[1] | 0.4952    |
| AverageReturn        | 199.09    |
| MinReturn            | 162       |
| MaxReturn            | 200       |
| StdReturn            | 4.962     |
| AverageEpisodeLength | 199.09    |
| MinEpisodeLength     | 162       |
| MaxEpisodeLength     | 200       |
| StdEpisodeLength     | 4.962     |
| TotalNEpisodes       | 383       |
| TotalNSamples        | 46664     |
| ExplainedVariance    | 0.75559   |
------------------------------------
[2018-09-05 09:05:53.648496 UTC] Saving snapshot
[2018-09-05 09:05:53.656466 UTC] Starting iteration 24
[2018-09-05 09:05:53.656615 UTC] Start collecting samples
[2018-09-05 09:05:53.958262 UTC] Computing input variables for policy optimization
[2018-09-05 09:05:53.979226 UTC] Computing policy gradient
[2018-09-05 09:05:53.989169 UTC] Updating baseline
[2018-09-05 09:05:54.115788 UTC] Computing logging information
-----------------------------------
| Iteration            | 24       |
| SurrLoss             | 0.007209 |
| Entropy              | 0.29359  |
| Perplexity           | 1.3412   |
| AveragePolicyProb[0] | 0.50031  |
| AveragePolicyProb[1] | 0.49969  |
| AverageReturn        | 199.86   |
| MinReturn            | 186      |
| MaxReturn            | 200      |
| StdReturn            | 1.393    |
| AverageEpisodeLength | 199.86   |
| MinEpisodeLength     | 186      |
| MaxEpisodeLength     | 200      |
| StdEpisodeLength     | 1.393    |
| TotalNEpisodes       | 390      |
| TotalNSamples        | 48064    |
| ExplainedVariance    | 0.77002  |
-----------------------------------
[2018-09-05 09:05:54.146781 UTC] Saving snapshot
[2018-09-05 09:05:54.155272 UTC] Starting iteration 25
[2018-09-05 09:05:54.155428 UTC] Start collecting samples
[2018-09-05 09:05:54.414859 UTC] Computing input variables for policy optimization
[2018-09-05 09:05:54.436791 UTC] Computing policy gradient
[2018-09-05 09:05:54.445548 UTC] Updating baseline
[2018-09-05 09:05:54.596484 UTC] Computing logging information
-----------------------------------
| Iteration            | 25       |
| SurrLoss             | -0.02172 |
| Entropy              | 0.29403  |
| Perplexity           | 1.3418   |
| AveragePolicyProb[0] | 0.48884  |
| AveragePolicyProb[1] | 0.51116  |
| AverageReturn        | 200      |
| MinReturn            | 200      |
| MaxReturn            | 200      |
| StdReturn            | 0        |
| AverageEpisodeLength | 200      |
| MinEpisodeLength     | 200      |
| MaxEpisodeLength     | 200      |
| StdEpisodeLength     | 0        |
| TotalNEpisodes       | 403      |
| TotalNSamples        | 50664    |
| ExplainedVariance    | 0.78151  |
-----------------------------------
[2018-09-05 09:05:54.629366 UTC] Saving snapshot
[2018-09-05 09:05:54.637303 UTC] Starting iteration 26
[2018-09-05 09:05:54.637453 UTC] Start collecting samples
[2018-09-05 09:05:54.905526 UTC] Computing input variables for policy optimization
[2018-09-05 09:05:54.929987 UTC] Computing policy gradient
[2018-09-05 09:05:54.939866 UTC] Updating baseline
[2018-09-05 09:05:55.065378 UTC] Computing logging information
-------------------------------------
| Iteration            | 26         |
| SurrLoss             | -0.0050363 |
| Entropy              | 0.28548    |
| Perplexity           | 1.3304     |
| AveragePolicyProb[0] | 0.50459    |
| AveragePolicyProb[1] | 0.49541    |
| AverageReturn        | 200        |
| MinReturn            | 200        |
| MaxReturn            | 200        |
| StdReturn            | 0          |
| AverageEpisodeLength | 200        |
| MinEpisodeLength     | 200        |
| MaxEpisodeLength     | 200        |
| StdEpisodeLength     | 0          |
| TotalNEpisodes       | 411        |
| TotalNSamples        | 52264      |
| ExplainedVariance    | 0.74011    |
-------------------------------------
[2018-09-05 09:05:55.099172 UTC] Saving snapshot
[2018-09-05 09:05:55.107078 UTC] Starting iteration 27
[2018-09-05 09:05:55.107228 UTC] Start collecting samples
[2018-09-05 09:05:55.354052 UTC] Computing input variables for policy optimization
[2018-09-05 09:05:55.374344 UTC] Computing policy gradient
[2018-09-05 09:05:55.383861 UTC] Updating baseline
[2018-09-05 09:05:55.515163 UTC] Computing logging information
-------------------------------------
| Iteration            | 27         |
| SurrLoss             | -0.0082108 |
| Entropy              | 0.2709     |
| Perplexity           | 1.3111     |
| AveragePolicyProb[0] | 0.50692    |
| AveragePolicyProb[1] | 0.49308    |
| AverageReturn        | 200        |
| MinReturn            | 200        |
| MaxReturn            | 200        |
| StdReturn            | 0          |
| AverageEpisodeLength | 200        |
| MinEpisodeLength     | 200        |
| MaxEpisodeLength     | 200        |
| StdEpisodeLength     | 0          |
| TotalNEpisodes       | 421        |
| TotalNSamples        | 54264      |
| ExplainedVariance    | 0.45897    |
-------------------------------------
[2018-09-05 09:05:55.546555 UTC] Saving snapshot
[2018-09-05 09:05:55.554449 UTC] Starting iteration 28
[2018-09-05 09:05:55.554596 UTC] Start collecting samples
[2018-09-05 09:05:55.819182 UTC] Computing input variables for policy optimization
[2018-09-05 09:05:55.844242 UTC] Computing policy gradient
[2018-09-05 09:05:55.856172 UTC] Updating baseline
[2018-09-05 09:05:55.991833 UTC] Computing logging information
-----------------------------------
| Iteration            | 28       |
| SurrLoss             | 0.010096 |
| Entropy              | 0.26801  |
| Perplexity           | 1.3074   |
| AveragePolicyProb[0] | 0.49824  |
| AveragePolicyProb[1] | 0.50176  |
| AverageReturn        | 200      |
| MinReturn            | 200      |
| MaxReturn            | 200      |
| StdReturn            | 0        |
| AverageEpisodeLength | 200      |
| MinEpisodeLength     | 200      |
| MaxEpisodeLength     | 200      |
| StdEpisodeLength     | 0        |
| TotalNEpisodes       | 433      |
| TotalNSamples        | 56664    |
| ExplainedVariance    | 0.5488   |
-----------------------------------
[2018-09-05 09:05:56.023951 UTC] Saving snapshot
[2018-09-05 09:05:56.031846 UTC] Starting iteration 29
[2018-09-05 09:05:56.031992 UTC] Start collecting samples
[2018-09-05 09:05:56.290403 UTC] Computing input variables for policy optimization
[2018-09-05 09:05:56.310092 UTC] Computing policy gradient
[2018-09-05 09:05:56.318869 UTC] Updating baseline
[2018-09-05 09:05:56.445542 UTC] Computing logging information
-------------------------------------
| Iteration            | 29         |
| SurrLoss             | -0.0043378 |
| Entropy              | 0.2686     |
| Perplexity           | 1.3081     |
| AveragePolicyProb[0] | 0.50192    |
| AveragePolicyProb[1] | 0.49808    |
| AverageReturn        | 200        |
| MinReturn            | 200        |
| MaxReturn            | 200        |
| StdReturn            | 0          |
| AverageEpisodeLength | 200        |
| MinEpisodeLength     | 200        |
| MaxEpisodeLength     | 200        |
| StdEpisodeLength     | 0          |
| TotalNEpisodes       | 442        |
| TotalNSamples        | 58464      |
| ExplainedVariance    | 0.3945     |
-------------------------------------
[2018-09-05 09:05:56.477746 UTC] Saving snapshot
[2018-09-05 09:05:56.485688 UTC] Starting iteration 30
[2018-09-05 09:05:56.485841 UTC] Start collecting samples
[2018-09-05 09:05:56.741218 UTC] Computing input variables for policy optimization
[2018-09-05 09:05:56.762124 UTC] Computing policy gradient
[2018-09-05 09:05:56.771676 UTC] Updating baseline
[2018-09-05 09:05:56.919854 UTC] Computing logging information
------------------------------------
| Iteration            | 30        |
| SurrLoss             | 0.0011018 |
| Entropy              | 0.26088   |
| Perplexity           | 1.2981    |
| AveragePolicyProb[0] | 0.50194   |
| AveragePolicyProb[1] | 0.49806   |
| AverageReturn        | 200       |
| MinReturn            | 200       |
| MaxReturn            | 200       |
| StdReturn            | 0         |
| AverageEpisodeLength | 200       |
| MinEpisodeLength     | 200       |
| MaxEpisodeLength     | 200       |
| StdEpisodeLength     | 0         |
| TotalNEpisodes       | 452       |
| TotalNSamples        | 60464     |
| ExplainedVariance    | 0.42767   |
------------------------------------
[2018-09-05 09:05:56.953929 UTC] Saving snapshot
[2018-09-05 09:05:56.962511 UTC] Starting iteration 31
[2018-09-05 09:05:56.962796 UTC] Start collecting samples
[2018-09-05 09:05:57.207949 UTC] Computing input variables for policy optimization
[2018-09-05 09:05:57.228610 UTC] Computing policy gradient
[2018-09-05 09:05:57.238020 UTC] Updating baseline
[2018-09-05 09:05:57.369271 UTC] Computing logging information
------------------------------------
| Iteration            | 31        |
| SurrLoss             | 0.0068073 |
| Entropy              | 0.2454    |
| Perplexity           | 1.2781    |
| AveragePolicyProb[0] | 0.50571   |
| AveragePolicyProb[1] | 0.49429   |
| AverageReturn        | 200       |
| MinReturn            | 200       |
| MaxReturn            | 200       |
| StdReturn            | 0         |
| AverageEpisodeLength | 200       |
| MinEpisodeLength     | 200       |
| MaxEpisodeLength     | 200       |
| StdEpisodeLength     | 0         |
| TotalNEpisodes       | 463       |
| TotalNSamples        | 62664     |
| ExplainedVariance    | 0.39212   |
------------------------------------
[2018-09-05 09:05:57.401917 UTC] Saving snapshot
[2018-09-05 09:05:57.409801 UTC] Starting iteration 32
[2018-09-05 09:05:57.409954 UTC] Start collecting samples
[2018-09-05 09:05:57.648749 UTC] Computing input variables for policy optimization
[2018-09-05 09:05:57.667598 UTC] Computing policy gradient
[2018-09-05 09:05:57.677938 UTC] Updating baseline
[2018-09-05 09:05:57.812430 UTC] Computing logging information
-------------------------------------
| Iteration            | 32         |
| SurrLoss             | 0.00078833 |
| Entropy              | 0.2543     |
| Perplexity           | 1.2896     |
| AveragePolicyProb[0] | 0.4942     |
| AveragePolicyProb[1] | 0.5058     |
| AverageReturn        | 200        |
| MinReturn            | 200        |
| MaxReturn            | 200        |
| StdReturn            | 0          |
| AverageEpisodeLength | 200        |
| MinEpisodeLength     | 200        |
| MaxEpisodeLength     | 200        |
| StdEpisodeLength     | 0          |
| TotalNEpisodes       | 470        |
| TotalNSamples        | 64064      |
| ExplainedVariance    | -0.29845   |
-------------------------------------
[2018-09-05 09:05:57.848674 UTC] Saving snapshot
[2018-09-05 09:05:57.856700 UTC] Starting iteration 33
[2018-09-05 09:05:57.856845 UTC] Start collecting samples
[2018-09-05 09:05:58.118808 UTC] Computing input variables for policy optimization
[2018-09-05 09:05:58.149311 UTC] Computing policy gradient
[2018-09-05 09:05:58.161947 UTC] Updating baseline
[2018-09-05 09:05:58.348568 UTC] Computing logging information
------------------------------------
| Iteration            | 33        |
| SurrLoss             | 0.0039279 |
| Entropy              | 0.2522    |
| Perplexity           | 1.2868    |
| AveragePolicyProb[0] | 0.5098    |
| AveragePolicyProb[1] | 0.4902    |
| AverageReturn        | 200       |
| MinReturn            | 200       |
| MaxReturn            | 200       |
| StdReturn            | 0         |
| AverageEpisodeLength | 200       |
| MinEpisodeLength     | 200       |
| MaxEpisodeLength     | 200       |
| StdEpisodeLength     | 0         |
| TotalNEpisodes       | 483       |
| TotalNSamples        | 66664     |
| ExplainedVariance    | -0.2062   |
------------------------------------
[2018-09-05 09:05:58.382004 UTC] Saving snapshot
[2018-09-05 09:05:58.389868 UTC] Starting iteration 34
[2018-09-05 09:05:58.390017 UTC] Start collecting samples
[2018-09-05 09:05:58.645016 UTC] Computing input variables for policy optimization
[2018-09-05 09:05:58.664041 UTC] Computing policy gradient
[2018-09-05 09:05:58.672769 UTC] Updating baseline
[2018-09-05 09:05:58.799814 UTC] Computing logging information
------------------------------------
| Iteration            | 34        |
| SurrLoss             | 0.0091203 |
| Entropy              | 0.25057   |
| Perplexity           | 1.2848    |
| AveragePolicyProb[0] | 0.49355   |
| AveragePolicyProb[1] | 0.50645   |
| AverageReturn        | 200       |
| MinReturn            | 200       |
| MaxReturn            | 200       |
| StdReturn            | 0         |
| AverageEpisodeLength | 200       |
| MinEpisodeLength     | 200       |
| MaxEpisodeLength     | 200       |
| StdEpisodeLength     | 0         |
| TotalNEpisodes       | 491       |
| TotalNSamples        | 68264     |
| ExplainedVariance    | 0.41054   |
------------------------------------
[2018-09-05 09:05:58.835288 UTC] Saving snapshot
[2018-09-05 09:05:58.843867 UTC] Starting iteration 35
[2018-09-05 09:05:58.844027 UTC] Start collecting samples
[2018-09-05 09:05:59.095007 UTC] Computing input variables for policy optimization
[2018-09-05 09:05:59.115607 UTC] Computing policy gradient
[2018-09-05 09:05:59.124709 UTC] Updating baseline
[2018-09-05 09:05:59.262673 UTC] Computing logging information
------------------------------------
| Iteration            | 35        |
| SurrLoss             | -0.025894 |
| Entropy              | 0.24312   |
| Perplexity           | 1.2752    |
| AveragePolicyProb[0] | 0.49555   |
| AveragePolicyProb[1] | 0.50445   |
| AverageReturn        | 200       |
| MinReturn            | 200       |
| MaxReturn            | 200       |
| StdReturn            | 0         |
| AverageEpisodeLength | 200       |
| MinEpisodeLength     | 200       |
| MaxEpisodeLength     | 200       |
| StdEpisodeLength     | 0         |
| TotalNEpisodes       | 501       |
| TotalNSamples        | 70264     |
| ExplainedVariance    | 0.60769   |
------------------------------------
[2018-09-05 09:05:59.297744 UTC] Saving snapshot
[2018-09-05 09:05:59.306402 UTC] Starting iteration 36
[2018-09-05 09:05:59.306555 UTC] Start collecting samples
[2018-09-05 09:05:59.559296 UTC] Computing input variables for policy optimization
[2018-09-05 09:05:59.581629 UTC] Computing policy gradient
[2018-09-05 09:05:59.590751 UTC] Updating baseline
[2018-09-05 09:05:59.718495 UTC] Computing logging information
-------------------------------------
| Iteration            | 36         |
| SurrLoss             | -0.0079505 |
| Entropy              | 0.24484    |
| Perplexity           | 1.2774     |
| AveragePolicyProb[0] | 0.49693    |
| AveragePolicyProb[1] | 0.50307    |
| AverageReturn        | 200        |
| MinReturn            | 200        |
| MaxReturn            | 200        |
| StdReturn            | 0          |
| AverageEpisodeLength | 200        |
| MinEpisodeLength     | 200        |
| MaxEpisodeLength     | 200        |
| StdEpisodeLength     | 0          |
| TotalNEpisodes       | 513        |
| TotalNSamples        | 72664      |
| ExplainedVariance    | 0.78295    |
-------------------------------------
[2018-09-05 09:05:59.751010 UTC] Saving snapshot
[2018-09-05 09:05:59.758894 UTC] Starting iteration 37
[2018-09-05 09:05:59.759041 UTC] Start collecting samples
[2018-09-05 09:06:00.023626 UTC] Computing input variables for policy optimization
[2018-09-05 09:06:00.047797 UTC] Computing policy gradient
[2018-09-05 09:06:00.057241 UTC] Updating baseline
[2018-09-05 09:06:00.186921 UTC] Computing logging information
------------------------------------
| Iteration            | 37        |
| SurrLoss             | -0.024677 |
| Entropy              | 0.23917   |
| Perplexity           | 1.2702    |
| AveragePolicyProb[0] | 0.49897   |
| AveragePolicyProb[1] | 0.50103   |
| AverageReturn        | 200       |
| MinReturn            | 200       |
| MaxReturn            | 200       |
| StdReturn            | 0         |
| AverageEpisodeLength | 200       |
| MinEpisodeLength     | 200       |
| MaxEpisodeLength     | 200       |
| StdEpisodeLength     | 0         |
| TotalNEpisodes       | 522       |
| TotalNSamples        | 74464     |
| ExplainedVariance    | 0.89268   |
------------------------------------
[2018-09-05 09:06:00.219180 UTC] Saving snapshot
[2018-09-05 09:06:00.227054 UTC] Starting iteration 38
[2018-09-05 09:06:00.227205 UTC] Start collecting samples
[2018-09-05 09:06:00.496605 UTC] Computing input variables for policy optimization
[2018-09-05 09:06:00.524398 UTC] Computing policy gradient
[2018-09-05 09:06:00.537292 UTC] Updating baseline
[2018-09-05 09:06:00.691978 UTC] Computing logging information
-------------------------------------
| Iteration            | 38         |
| SurrLoss             | -0.0080315 |
| Entropy              | 0.23749    |
| Perplexity           | 1.2681     |
| AveragePolicyProb[0] | 0.51161    |
| AveragePolicyProb[1] | 0.48839    |
| AverageReturn        | 200        |
| MinReturn            | 200        |
| MaxReturn            | 200        |
| StdReturn            | 0          |
| AverageEpisodeLength | 200        |
| MinEpisodeLength     | 200        |
| MaxEpisodeLength     | 200        |
| StdEpisodeLength     | 0          |
| TotalNEpisodes       | 532        |
| TotalNSamples        | 76464      |
| ExplainedVariance    | 0.8846     |
-------------------------------------
[2018-09-05 09:06:00.726731 UTC] Saving snapshot
[2018-09-05 09:06:00.734697 UTC] Starting iteration 39
[2018-09-05 09:06:00.734841 UTC] Start collecting samples
[2018-09-05 09:06:01.038317 UTC] Computing input variables for policy optimization
[2018-09-05 09:06:01.064154 UTC] Computing policy gradient
[2018-09-05 09:06:01.076000 UTC] Updating baseline
[2018-09-05 09:06:01.190831 UTC] Computing logging information
-------------------------------------
| Iteration            | 39         |
| SurrLoss             | -0.0089631 |
| Entropy              | 0.22513    |
| Perplexity           | 1.2525     |
| AveragePolicyProb[0] | 0.49868    |
| AveragePolicyProb[1] | 0.50132    |
| AverageReturn        | 200        |
| MinReturn            | 200        |
| MaxReturn            | 200        |
| StdReturn            | 0          |
| AverageEpisodeLength | 200        |
| MinEpisodeLength     | 200        |
| MaxEpisodeLength     | 200        |
| StdEpisodeLength     | 0          |
| TotalNEpisodes       | 543        |
| TotalNSamples        | 78664      |
| ExplainedVariance    | 0.76064    |
-------------------------------------
[2018-09-05 09:06:01.223444 UTC] Saving snapshot
[2018-09-05 09:06:01.231971 UTC] Starting iteration 40
[2018-09-05 09:06:01.232123 UTC] Start collecting samples
[2018-09-05 09:06:01.481514 UTC] Computing input variables for policy optimization
[2018-09-05 09:06:01.499638 UTC] Computing policy gradient
[2018-09-05 09:06:01.508728 UTC] Updating baseline
[2018-09-05 09:06:01.627997 UTC] Computing logging information
------------------------------------
| Iteration            | 40        |
| SurrLoss             | -0.033313 |
| Entropy              | 0.22582   |
| Perplexity           | 1.2534    |
| AveragePolicyProb[0] | 0.50311   |
| AveragePolicyProb[1] | 0.49689   |
| AverageReturn        | 200       |
| MinReturn            | 200       |
| MaxReturn            | 200       |
| StdReturn            | 0         |
| AverageEpisodeLength | 200       |
| MinEpisodeLength     | 200       |
| MaxEpisodeLength     | 200       |
| StdEpisodeLength     | 0         |
| TotalNEpisodes       | 550       |
| TotalNSamples        | 80064     |
| ExplainedVariance    | 0.80032   |
------------------------------------
[2018-09-05 09:06:01.661283 UTC] Saving snapshot
[2018-09-05 09:06:01.669476 UTC] Starting iteration 41
[2018-09-05 09:06:01.669637 UTC] Start collecting samples
[2018-09-05 09:06:01.941997 UTC] Computing input variables for policy optimization
[2018-09-05 09:06:01.964117 UTC] Computing policy gradient
[2018-09-05 09:06:01.974489 UTC] Updating baseline
[2018-09-05 09:06:02.092051 UTC] Computing logging information
------------------------------------
| Iteration            | 41        |
| SurrLoss             | 0.0058418 |
| Entropy              | 0.21298   |
| Perplexity           | 1.2374    |
| AveragePolicyProb[0] | 0.48765   |
| AveragePolicyProb[1] | 0.51235   |
| AverageReturn        | 200       |
| MinReturn            | 200       |
| MaxReturn            | 200       |
| StdReturn            | 0         |
| AverageEpisodeLength | 200       |
| MinEpisodeLength     | 200       |
| MaxEpisodeLength     | 200       |
| StdEpisodeLength     | 0         |
| TotalNEpisodes       | 563       |
| TotalNSamples        | 82664     |
| ExplainedVariance    | 0.64831   |
------------------------------------
[2018-09-05 09:06:02.126018 UTC] Saving snapshot
[2018-09-05 09:06:02.134560 UTC] Starting iteration 42
[2018-09-05 09:06:02.134711 UTC] Start collecting samples
[2018-09-05 09:06:02.392304 UTC] Computing input variables for policy optimization
[2018-09-05 09:06:02.411570 UTC] Computing policy gradient
[2018-09-05 09:06:02.420871 UTC] Updating baseline
[2018-09-05 09:06:02.556046 UTC] Computing logging information
-----------------------------------
| Iteration            | 42       |
| SurrLoss             | 0.015621 |
| Entropy              | 0.21475  |
| Perplexity           | 1.2395   |
| AveragePolicyProb[0] | 0.5014   |
| AveragePolicyProb[1] | 0.49861  |
| AverageReturn        | 200      |
| MinReturn            | 200      |
| MaxReturn            | 200      |
| StdReturn            | 0        |
| AverageEpisodeLength | 200      |
| MinEpisodeLength     | 200      |
| MaxEpisodeLength     | 200      |
| StdEpisodeLength     | 0        |
| TotalNEpisodes       | 571      |
| TotalNSamples        | 84264    |
| ExplainedVariance    | 0.51792  |
-----------------------------------
[2018-09-05 09:06:02.595775 UTC] Saving snapshot
[2018-09-05 09:06:02.603708 UTC] Starting iteration 43
[2018-09-05 09:06:02.603854 UTC] Start collecting samples
[2018-09-05 09:06:02.862052 UTC] Computing input variables for policy optimization
[2018-09-05 09:06:02.882715 UTC] Computing policy gradient
[2018-09-05 09:06:02.892317 UTC] Updating baseline
[2018-09-05 09:06:03.023168 UTC] Computing logging information
------------------------------------
| Iteration            | 43        |
| SurrLoss             | -0.007604 |
| Entropy              | 0.20815   |
| Perplexity           | 1.2314    |
| AveragePolicyProb[0] | 0.50453   |
| AveragePolicyProb[1] | 0.49547   |
| AverageReturn        | 200       |
| MinReturn            | 200       |
| MaxReturn            | 200       |
| StdReturn            | 0         |
| AverageEpisodeLength | 200       |
| MinEpisodeLength     | 200       |
| MaxEpisodeLength     | 200       |
| StdEpisodeLength     | 0         |
| TotalNEpisodes       | 581       |
| TotalNSamples        | 86264     |
| ExplainedVariance    | 0.43089   |
------------------------------------
[2018-09-05 09:06:03.059032 UTC] Saving snapshot
[2018-09-05 09:06:03.067542 UTC] Starting iteration 44
[2018-09-05 09:06:03.067687 UTC] Start collecting samples
[2018-09-05 09:06:03.319435 UTC] Computing input variables for policy optimization
[2018-09-05 09:06:03.348216 UTC] Computing policy gradient
[2018-09-05 09:06:03.361170 UTC] Updating baseline
[2018-09-05 09:06:03.543482 UTC] Computing logging information
-----------------------------------
| Iteration            | 44       |
| SurrLoss             | -0.01006 |
| Entropy              | 0.21299  |
| Perplexity           | 1.2374   |
| AveragePolicyProb[0] | 0.49897  |
| AveragePolicyProb[1] | 0.50103  |
| AverageReturn        | 200      |
| MinReturn            | 200      |
| MaxReturn            | 200      |
| StdReturn            | 0        |
| AverageEpisodeLength | 200      |
| MinEpisodeLength     | 200      |
| MaxEpisodeLength     | 200      |
| StdEpisodeLength     | 0        |
| TotalNEpisodes       | 593      |
| TotalNSamples        | 88664    |
| ExplainedVariance    | 0.32192  |
-----------------------------------
[2018-09-05 09:06:03.590932 UTC] Saving snapshot
[2018-09-05 09:06:03.601952 UTC] Starting iteration 45
[2018-09-05 09:06:03.602133 UTC] Start collecting samples
[2018-09-05 09:06:03.886736 UTC] Computing input variables for policy optimization
[2018-09-05 09:06:03.913695 UTC] Computing policy gradient
[2018-09-05 09:06:03.926018 UTC] Updating baseline
[2018-09-05 09:06:04.094618 UTC] Computing logging information
-----------------------------------
| Iteration            | 45       |
| SurrLoss             | 0.015187 |
| Entropy              | 0.20911  |
| Perplexity           | 1.2326   |
| AveragePolicyProb[0] | 0.48749  |
| AveragePolicyProb[1] | 0.51251  |
| AverageReturn        | 200      |
| MinReturn            | 200      |
| MaxReturn            | 200      |
| StdReturn            | 0        |
| AverageEpisodeLength | 200      |
| MinEpisodeLength     | 200      |
| MaxEpisodeLength     | 200      |
| StdEpisodeLength     | 0        |
| TotalNEpisodes       | 602      |
| TotalNSamples        | 90464    |
| ExplainedVariance    | 0.28443  |
-----------------------------------
[2018-09-05 09:06:04.128742 UTC] Saving snapshot
[2018-09-05 09:06:04.137177 UTC] Starting iteration 46
[2018-09-05 09:06:04.137326 UTC] Start collecting samples
[2018-09-05 09:06:04.392244 UTC] Computing input variables for policy optimization
[2018-09-05 09:06:04.412365 UTC] Computing policy gradient
[2018-09-05 09:06:04.422511 UTC] Updating baseline
[2018-09-05 09:06:04.555113 UTC] Computing logging information
-------------------------------------
| Iteration            | 46         |
| SurrLoss             | -0.0091266 |
| Entropy              | 0.21489    |
| Perplexity           | 1.2397     |
| AveragePolicyProb[0] | 0.50331    |
| AveragePolicyProb[1] | 0.49669    |
| AverageReturn        | 200        |
| MinReturn            | 200        |
| MaxReturn            | 200        |
| StdReturn            | 0          |
| AverageEpisodeLength | 200        |
| MinEpisodeLength     | 200        |
| MaxEpisodeLength     | 200        |
| StdEpisodeLength     | 0          |
| TotalNEpisodes       | 612        |
| TotalNSamples        | 92464      |
| ExplainedVariance    | -0.014127  |
-------------------------------------
[2018-09-05 09:06:04.590509 UTC] Saving snapshot
[2018-09-05 09:06:04.598531 UTC] Starting iteration 47
[2018-09-05 09:06:04.598680 UTC] Start collecting samples
[2018-09-05 09:06:04.859803 UTC] Computing input variables for policy optimization
[2018-09-05 09:06:04.881322 UTC] Computing policy gradient
[2018-09-05 09:06:04.890556 UTC] Updating baseline
[2018-09-05 09:06:05.024429 UTC] Computing logging information
------------------------------------
| Iteration            | 47        |
| SurrLoss             | -0.016498 |
| Entropy              | 0.23502   |
| Perplexity           | 1.2649    |
| AveragePolicyProb[0] | 0.50672   |
| AveragePolicyProb[1] | 0.49328   |
| AverageReturn        | 200       |
| MinReturn            | 200       |
| MaxReturn            | 200       |
| StdReturn            | 0         |
| AverageEpisodeLength | 200       |
| MinEpisodeLength     | 200       |
| MaxEpisodeLength     | 200       |
| StdEpisodeLength     | 0         |
| TotalNEpisodes       | 623       |
| TotalNSamples        | 94664     |
| ExplainedVariance    | -0.015993 |
------------------------------------
[2018-09-05 09:06:05.058445 UTC] Saving snapshot
[2018-09-05 09:06:05.066460 UTC] Starting iteration 48
[2018-09-05 09:06:05.066607 UTC] Start collecting samples
[2018-09-05 09:06:05.327656 UTC] Computing input variables for policy optimization
[2018-09-05 09:06:05.346833 UTC] Computing policy gradient
[2018-09-05 09:06:05.356167 UTC] Updating baseline
[2018-09-05 09:06:05.517394 UTC] Computing logging information
-----------------------------------
| Iteration            | 48       |
| SurrLoss             | 0.026595 |
| Entropy              | 0.2394   |
| Perplexity           | 1.2705   |
| AveragePolicyProb[0] | 0.50154  |
| AveragePolicyProb[1] | 0.49846  |
| AverageReturn        | 200      |
| MinReturn            | 200      |
| MaxReturn            | 200      |
| StdReturn            | 0        |
| AverageEpisodeLength | 200      |
| MinEpisodeLength     | 200      |
| MaxEpisodeLength     | 200      |
| StdEpisodeLength     | 0        |
| TotalNEpisodes       | 630      |
| TotalNSamples        | 96064    |
| ExplainedVariance    | -0.20233 |
-----------------------------------
[2018-09-05 09:06:05.552355 UTC] Saving snapshot
[2018-09-05 09:06:05.560480 UTC] Starting iteration 49
[2018-09-05 09:06:05.560631 UTC] Start collecting samples
[2018-09-05 09:06:05.823299 UTC] Computing input variables for policy optimization
[2018-09-05 09:06:05.848138 UTC] Computing policy gradient
[2018-09-05 09:06:05.857816 UTC] Updating baseline
[2018-09-05 09:06:06.005518 UTC] Computing logging information
------------------------------------
| Iteration            | 49        |
| SurrLoss             | 0.0029137 |
| Entropy              | 0.24121   |
| Perplexity           | 1.2728    |
| AveragePolicyProb[0] | 0.51613   |
| AveragePolicyProb[1] | 0.48387   |
| AverageReturn        | 200       |
| MinReturn            | 200       |
| MaxReturn            | 200       |
| StdReturn            | 0         |
| AverageEpisodeLength | 200       |
| MinEpisodeLength     | 200       |
| MaxEpisodeLength     | 200       |
| StdEpisodeLength     | 0         |
| TotalNEpisodes       | 643       |
| TotalNSamples        | 98664     |
| ExplainedVariance    | -0.059967 |
------------------------------------
[2018-09-05 09:06:06.041961 UTC] Saving snapshot
[2018-09-05 09:06:06.049867 UTC] Starting iteration 50
[2018-09-05 09:06:06.050014 UTC] Start collecting samples
[2018-09-05 09:06:06.313460 UTC] Computing input variables for policy optimization
[2018-09-05 09:06:06.338862 UTC] Computing policy gradient
[2018-09-05 09:06:06.348807 UTC] Updating baseline
[2018-09-05 09:06:06.482650 UTC] Computing logging information
-------------------------------------
| Iteration            | 50         |
| SurrLoss             | -0.014804  |
| Entropy              | 0.22605    |
| Perplexity           | 1.2536     |
| AveragePolicyProb[0] | 0.49513    |
| AveragePolicyProb[1] | 0.50487    |
| AverageReturn        | 200        |
| MinReturn            | 200        |
| MaxReturn            | 200        |
| StdReturn            | 0          |
| AverageEpisodeLength | 200        |
| MinEpisodeLength     | 200        |
| MaxEpisodeLength     | 200        |
| StdEpisodeLength     | 0          |
| TotalNEpisodes       | 651        |
| TotalNSamples        | 1.0026e+05 |
| ExplainedVariance    | 0.074192   |
-------------------------------------
[2018-09-05 09:06:06.528473 UTC] Saving snapshot
[2018-09-05 09:06:06.539536 UTC] Starting iteration 51
[2018-09-05 09:06:06.539716 UTC] Start collecting samples
[2018-09-05 09:06:06.800335 UTC] Computing input variables for policy optimization
[2018-09-05 09:06:06.821275 UTC] Computing policy gradient
[2018-09-05 09:06:06.830271 UTC] Updating baseline
[2018-09-05 09:06:06.973147 UTC] Computing logging information
-------------------------------------
| Iteration            | 51         |
| SurrLoss             | -0.012072  |
| Entropy              | 0.23508    |
| Perplexity           | 1.265      |
| AveragePolicyProb[0] | 0.50575    |
| AveragePolicyProb[1] | 0.49425    |
| AverageReturn        | 200        |
| MinReturn            | 200        |
| MaxReturn            | 200        |
| StdReturn            | 0          |
| AverageEpisodeLength | 200        |
| MinEpisodeLength     | 200        |
| MaxEpisodeLength     | 200        |
| StdEpisodeLength     | 0          |
| TotalNEpisodes       | 661        |
| TotalNSamples        | 1.0226e+05 |
| ExplainedVariance    | 0.18737    |
-------------------------------------
[2018-09-05 09:06:07.009888 UTC] Saving snapshot
[2018-09-05 09:06:07.017719 UTC] Starting iteration 52
[2018-09-05 09:06:07.017868 UTC] Start collecting samples
[2018-09-05 09:06:07.291222 UTC] Computing input variables for policy optimization
[2018-09-05 09:06:07.320088 UTC] Computing policy gradient
[2018-09-05 09:06:07.331270 UTC] Updating baseline
[2018-09-05 09:06:07.465023 UTC] Computing logging information
-------------------------------------
| Iteration            | 52         |
| SurrLoss             | -0.0050887 |
| Entropy              | 0.23794    |
| Perplexity           | 1.2686     |
| AveragePolicyProb[0] | 0.49711    |
| AveragePolicyProb[1] | 0.50289    |
| AverageReturn        | 200        |
| MinReturn            | 200        |
| MaxReturn            | 200        |
| StdReturn            | 0          |
| AverageEpisodeLength | 200        |
| MinEpisodeLength     | 200        |
| MaxEpisodeLength     | 200        |
| StdEpisodeLength     | 0          |
| TotalNEpisodes       | 673        |
| TotalNSamples        | 1.0466e+05 |
| ExplainedVariance    | 0.15245    |
-------------------------------------
[2018-09-05 09:06:07.499095 UTC] Saving snapshot
[2018-09-05 09:06:07.506949 UTC] Starting iteration 53
[2018-09-05 09:06:07.507100 UTC] Start collecting samples
[2018-09-05 09:06:07.757951 UTC] Computing input variables for policy optimization
[2018-09-05 09:06:07.785033 UTC] Computing policy gradient
[2018-09-05 09:06:07.798165 UTC] Updating baseline
[2018-09-05 09:06:07.958408 UTC] Computing logging information
-------------------------------------
| Iteration            | 53         |
| SurrLoss             | -0.012112  |
| Entropy              | 0.2356     |
| Perplexity           | 1.2657     |
| AveragePolicyProb[0] | 0.5104     |
| AveragePolicyProb[1] | 0.4896     |
| AverageReturn        | 200        |
| MinReturn            | 200        |
| MaxReturn            | 200        |
| StdReturn            | 0          |
| AverageEpisodeLength | 200        |
| MinEpisodeLength     | 200        |
| MaxEpisodeLength     | 200        |
| StdEpisodeLength     | 0          |
| TotalNEpisodes       | 682        |
| TotalNSamples        | 1.0646e+05 |
| ExplainedVariance    | 0.42059    |
-------------------------------------
[2018-09-05 09:06:07.995385 UTC] Saving snapshot
[2018-09-05 09:06:08.003915 UTC] Starting iteration 54
[2018-09-05 09:06:08.004062 UTC] Start collecting samples
[2018-09-05 09:06:08.258453 UTC] Computing input variables for policy optimization
[2018-09-05 09:06:08.282330 UTC] Computing policy gradient
[2018-09-05 09:06:08.291582 UTC] Updating baseline
[2018-09-05 09:06:08.417741 UTC] Computing logging information
-------------------------------------
| Iteration            | 54         |
| SurrLoss             | -0.0068054 |
| Entropy              | 0.2223     |
| Perplexity           | 1.249      |
| AveragePolicyProb[0] | 0.49772    |
| AveragePolicyProb[1] | 0.50228    |
| AverageReturn        | 200        |
| MinReturn            | 200        |
| MaxReturn            | 200        |
| StdReturn            | 0          |
| AverageEpisodeLength | 200        |
| MinEpisodeLength     | 200        |
| MaxEpisodeLength     | 200        |
| StdEpisodeLength     | 0          |
| TotalNEpisodes       | 692        |
| TotalNSamples        | 1.0846e+05 |
| ExplainedVariance    | 0.62698    |
-------------------------------------
[2018-09-05 09:06:08.453376 UTC] Saving snapshot
[2018-09-05 09:06:08.461343 UTC] Starting iteration 55
[2018-09-05 09:06:08.461493 UTC] Start collecting samples
[2018-09-05 09:06:08.727352 UTC] Computing input variables for policy optimization
[2018-09-05 09:06:08.748355 UTC] Computing policy gradient
[2018-09-05 09:06:08.758102 UTC] Updating baseline
[2018-09-05 09:06:08.921501 UTC] Computing logging information
-------------------------------------
| Iteration            | 55         |
| SurrLoss             | -0.0030681 |
| Entropy              | 0.22405    |
| Perplexity           | 1.2511     |
| AveragePolicyProb[0] | 0.4965     |
| AveragePolicyProb[1] | 0.5035     |
| AverageReturn        | 200        |
| MinReturn            | 200        |
| MaxReturn            | 200        |
| StdReturn            | 0          |
| AverageEpisodeLength | 200        |
| MinEpisodeLength     | 200        |
| MaxEpisodeLength     | 200        |
| StdEpisodeLength     | 0          |
| TotalNEpisodes       | 703        |
| TotalNSamples        | 1.1066e+05 |
| ExplainedVariance    | 0.29886    |
-------------------------------------
[2018-09-05 09:06:08.959083 UTC] Saving snapshot
[2018-09-05 09:06:08.966994 UTC] Starting iteration 56
[2018-09-05 09:06:08.967142 UTC] Start collecting samples
[2018-09-05 09:06:09.224542 UTC] Computing input variables for policy optimization
[2018-09-05 09:06:09.243032 UTC] Computing policy gradient
[2018-09-05 09:06:09.251800 UTC] Updating baseline
[2018-09-05 09:06:09.389987 UTC] Computing logging information
-------------------------------------
| Iteration            | 56         |
| SurrLoss             | -0.007297  |
| Entropy              | 0.21953    |
| Perplexity           | 1.2455     |
| AveragePolicyProb[0] | 0.48938    |
| AveragePolicyProb[1] | 0.51062    |
| AverageReturn        | 200        |
| MinReturn            | 200        |
| MaxReturn            | 200        |
| StdReturn            | 0          |
| AverageEpisodeLength | 200        |
| MinEpisodeLength     | 200        |
| MaxEpisodeLength     | 200        |
| StdEpisodeLength     | 0          |
| TotalNEpisodes       | 710        |
| TotalNSamples        | 1.1206e+05 |
| ExplainedVariance    | 0.20535    |
-------------------------------------
[2018-09-05 09:06:09.427351 UTC] Saving snapshot
[2018-09-05 09:06:09.435443 UTC] Starting iteration 57
[2018-09-05 09:06:09.435597 UTC] Start collecting samples
[2018-09-05 09:06:09.712418 UTC] Computing input variables for policy optimization
[2018-09-05 09:06:09.742634 UTC] Computing policy gradient
[2018-09-05 09:06:09.751678 UTC] Updating baseline
[2018-09-05 09:06:09.868535 UTC] Computing logging information
-------------------------------------
| Iteration            | 57         |
| SurrLoss             | -0.012438  |
| Entropy              | 0.21395    |
| Perplexity           | 1.2386     |
| AveragePolicyProb[0] | 0.49857    |
| AveragePolicyProb[1] | 0.50143    |
| AverageReturn        | 200        |
| MinReturn            | 200        |
| MaxReturn            | 200        |
| StdReturn            | 0          |
| AverageEpisodeLength | 200        |
| MinEpisodeLength     | 200        |
| MaxEpisodeLength     | 200        |
| StdEpisodeLength     | 0          |
| TotalNEpisodes       | 723        |
| TotalNSamples        | 1.1466e+05 |
| ExplainedVariance    | 0.2071     |
-------------------------------------
[2018-09-05 09:06:09.904821 UTC] Saving snapshot
[2018-09-05 09:06:09.912694 UTC] Starting iteration 58
[2018-09-05 09:06:09.912839 UTC] Start collecting samples
[2018-09-05 09:06:10.181100 UTC] Computing input variables for policy optimization
[2018-09-05 09:06:10.200880 UTC] Computing policy gradient
[2018-09-05 09:06:10.209875 UTC] Updating baseline
[2018-09-05 09:06:10.356086 UTC] Computing logging information
-------------------------------------
| Iteration            | 58         |
| SurrLoss             | 0.0053383  |
| Entropy              | 0.20469    |
| Perplexity           | 1.2271     |
| AveragePolicyProb[0] | 0.50142    |
| AveragePolicyProb[1] | 0.49858    |
| AverageReturn        | 200        |
| MinReturn            | 200        |
| MaxReturn            | 200        |
| StdReturn            | 0          |
| AverageEpisodeLength | 200        |
| MinEpisodeLength     | 200        |
| MaxEpisodeLength     | 200        |
| StdEpisodeLength     | 0          |
| TotalNEpisodes       | 731        |
| TotalNSamples        | 1.1626e+05 |
| ExplainedVariance    | 0.039218   |
-------------------------------------
[2018-09-05 09:06:10.395927 UTC] Saving snapshot
[2018-09-05 09:06:10.404459 UTC] Starting iteration 59
[2018-09-05 09:06:10.404608 UTC] Start collecting samples
[2018-09-05 09:06:10.749985 UTC] Computing input variables for policy optimization
[2018-09-05 09:06:10.774010 UTC] Computing policy gradient
[2018-09-05 09:06:10.786555 UTC] Updating baseline
[2018-09-05 09:06:10.934149 UTC] Computing logging information
-------------------------------------
| Iteration            | 59         |
| SurrLoss             | 0.014476   |
| Entropy              | 0.19275    |
| Perplexity           | 1.2126     |
| AveragePolicyProb[0] | 0.5053     |
| AveragePolicyProb[1] | 0.4947     |
| AverageReturn        | 200        |
| MinReturn            | 200        |
| MaxReturn            | 200        |
| StdReturn            | 0          |
| AverageEpisodeLength | 200        |
| MinEpisodeLength     | 200        |
| MaxEpisodeLength     | 200        |
| StdEpisodeLength     | 0          |
| TotalNEpisodes       | 741        |
| TotalNSamples        | 1.1826e+05 |
| ExplainedVariance    | -0.072357  |
-------------------------------------
[2018-09-05 09:06:10.972128 UTC] Saving snapshot
[2018-09-05 09:06:10.980044 UTC] Starting iteration 60
[2018-09-05 09:06:10.980196 UTC] Start collecting samples
[2018-09-05 09:06:11.245404 UTC] Computing input variables for policy optimization
[2018-09-05 09:06:11.266931 UTC] Computing policy gradient
[2018-09-05 09:06:11.276591 UTC] Updating baseline
[2018-09-05 09:06:11.437185 UTC] Computing logging information
-------------------------------------
| Iteration            | 60         |
| SurrLoss             | -0.0087871 |
| Entropy              | 0.20436    |
| Perplexity           | 1.2267     |
| AveragePolicyProb[0] | 0.50792    |
| AveragePolicyProb[1] | 0.49208    |
| AverageReturn        | 200        |
| MinReturn            | 200        |
| MaxReturn            | 200        |
| StdReturn            | 0          |
| AverageEpisodeLength | 200        |
| MinEpisodeLength     | 200        |
| MaxEpisodeLength     | 200        |
| StdEpisodeLength     | 0          |
| TotalNEpisodes       | 753        |
| TotalNSamples        | 1.2066e+05 |
| ExplainedVariance    | 0.04495    |
-------------------------------------
[2018-09-05 09:06:11.474360 UTC] Saving snapshot
[2018-09-05 09:06:11.482408 UTC] Starting iteration 61
[2018-09-05 09:06:11.482559 UTC] Start collecting samples
[2018-09-05 09:06:11.743762 UTC] Computing input variables for policy optimization
[2018-09-05 09:06:11.763600 UTC] Computing policy gradient
[2018-09-05 09:06:11.772618 UTC] Updating baseline
[2018-09-05 09:06:11.908536 UTC] Computing logging information
-------------------------------------
| Iteration            | 61         |
| SurrLoss             | -0.029418  |
| Entropy              | 0.19842    |
| Perplexity           | 1.2195     |
| AveragePolicyProb[0] | 0.49525    |
| AveragePolicyProb[1] | 0.50475    |
| AverageReturn        | 200        |
| MinReturn            | 200        |
| MaxReturn            | 200        |
| StdReturn            | 0          |
| AverageEpisodeLength | 200        |
| MinEpisodeLength     | 200        |
| MaxEpisodeLength     | 200        |
| StdEpisodeLength     | 0          |
| TotalNEpisodes       | 762        |
| TotalNSamples        | 1.2246e+05 |
| ExplainedVariance    | 0.27389    |
-------------------------------------
[2018-09-05 09:06:11.946729 UTC] Saving snapshot
[2018-09-05 09:06:11.954754 UTC] Starting iteration 62
[2018-09-05 09:06:11.954904 UTC] Start collecting samples
[2018-09-05 09:06:12.207991 UTC] Computing input variables for policy optimization
[2018-09-05 09:06:12.229397 UTC] Computing policy gradient
[2018-09-05 09:06:12.238409 UTC] Updating baseline
[2018-09-05 09:06:12.375738 UTC] Computing logging information
-------------------------------------
| Iteration            | 62         |
| SurrLoss             | -0.0095828 |
| Entropy              | 0.20144    |
| Perplexity           | 1.2232     |
| AveragePolicyProb[0] | 0.5044     |
| AveragePolicyProb[1] | 0.4956     |
| AverageReturn        | 200        |
| MinReturn            | 200        |
| MaxReturn            | 200        |
| StdReturn            | 0          |
| AverageEpisodeLength | 200        |
| MinEpisodeLength     | 200        |
| MaxEpisodeLength     | 200        |
| StdEpisodeLength     | 0          |
| TotalNEpisodes       | 772        |
| TotalNSamples        | 1.2446e+05 |
| ExplainedVariance    | 0.48951    |
-------------------------------------
[2018-09-05 09:06:12.411660 UTC] Saving snapshot
[2018-09-05 09:06:12.420136 UTC] Starting iteration 63
[2018-09-05 09:06:12.420290 UTC] Start collecting samples
[2018-09-05 09:06:12.674403 UTC] Computing input variables for policy optimization
[2018-09-05 09:06:12.697204 UTC] Computing policy gradient
[2018-09-05 09:06:12.706774 UTC] Updating baseline
[2018-09-05 09:06:12.838786 UTC] Computing logging information
-------------------------------------
| Iteration            | 63         |
| SurrLoss             | 0.0044326  |
| Entropy              | 0.19828    |
| Perplexity           | 1.2193     |
| AveragePolicyProb[0] | 0.49973    |
| AveragePolicyProb[1] | 0.50027    |
| AverageReturn        | 200        |
| MinReturn            | 200        |
| MaxReturn            | 200        |
| StdReturn            | 0          |
| AverageEpisodeLength | 200        |
| MinEpisodeLength     | 200        |
| MaxEpisodeLength     | 200        |
| StdEpisodeLength     | 0          |
| TotalNEpisodes       | 783        |
| TotalNSamples        | 1.2666e+05 |
| ExplainedVariance    | 0.62932    |
-------------------------------------
[2018-09-05 09:06:12.877266 UTC] Saving snapshot
[2018-09-05 09:06:12.885461 UTC] Starting iteration 64
[2018-09-05 09:06:12.885744 UTC] Start collecting samples
[2018-09-05 09:06:13.156639 UTC] Computing input variables for policy optimization
[2018-09-05 09:06:13.175030 UTC] Computing policy gradient
[2018-09-05 09:06:13.184497 UTC] Updating baseline
[2018-09-05 09:06:13.321073 UTC] Computing logging information
-------------------------------------
| Iteration            | 64         |
| SurrLoss             | -0.0087411 |
| Entropy              | 0.20427    |
| Perplexity           | 1.2266     |
| AveragePolicyProb[0] | 0.51224    |
| AveragePolicyProb[1] | 0.48776    |
| AverageReturn        | 200        |
| MinReturn            | 200        |
| MaxReturn            | 200        |
| StdReturn            | 0          |
| AverageEpisodeLength | 200        |
| MinEpisodeLength     | 200        |
| MaxEpisodeLength     | 200        |
| StdEpisodeLength     | 0          |
| TotalNEpisodes       | 790        |
| TotalNSamples        | 1.2806e+05 |
| ExplainedVariance    | 0.53214    |
-------------------------------------
[2018-09-05 09:06:13.358532 UTC] Saving snapshot
[2018-09-05 09:06:13.367027 UTC] Starting iteration 65
[2018-09-05 09:06:13.367182 UTC] Start collecting samples
[2018-09-05 09:06:13.641318 UTC] Computing input variables for policy optimization
[2018-09-05 09:06:13.671588 UTC] Computing policy gradient
[2018-09-05 09:06:13.683822 UTC] Updating baseline
[2018-09-05 09:06:13.890481 UTC] Computing logging information
-------------------------------------
| Iteration            | 65         |
| SurrLoss             | -0.0081594 |
| Entropy              | 0.20625    |
| Perplexity           | 1.2291     |
| AveragePolicyProb[0] | 0.50401    |
| AveragePolicyProb[1] | 0.49599    |
| AverageReturn        | 200        |
| MinReturn            | 200        |
| MaxReturn            | 200        |
| StdReturn            | 0          |
| AverageEpisodeLength | 200        |
| MinEpisodeLength     | 200        |
| MaxEpisodeLength     | 200        |
| StdEpisodeLength     | 0          |
| TotalNEpisodes       | 803        |
| TotalNSamples        | 1.3066e+05 |
| ExplainedVariance    | 0.55224    |
-------------------------------------
[2018-09-05 09:06:13.927795 UTC] Saving snapshot
[2018-09-05 09:06:13.935979 UTC] Starting iteration 66
[2018-09-05 09:06:13.936124 UTC] Start collecting samples
[2018-09-05 09:06:14.198212 UTC] Computing input variables for policy optimization
[2018-09-05 09:06:14.224461 UTC] Computing policy gradient
[2018-09-05 09:06:14.236856 UTC] Updating baseline
[2018-09-05 09:06:14.419926 UTC] Computing logging information
-------------------------------------
| Iteration            | 66         |
| SurrLoss             | -0.013821  |
| Entropy              | 0.19783    |
| Perplexity           | 1.2188     |
| AveragePolicyProb[0] | 0.49978    |
| AveragePolicyProb[1] | 0.50022    |
| AverageReturn        | 200        |
| MinReturn            | 200        |
| MaxReturn            | 200        |
| StdReturn            | 0          |
| AverageEpisodeLength | 200        |
| MinEpisodeLength     | 200        |
| MaxEpisodeLength     | 200        |
| StdEpisodeLength     | 0          |
| TotalNEpisodes       | 811        |
| TotalNSamples        | 1.3226e+05 |
| ExplainedVariance    | 0.65122    |
-------------------------------------
[2018-09-05 09:06:14.457766 UTC] Saving snapshot
[2018-09-05 09:06:14.466712 UTC] Starting iteration 67
[2018-09-05 09:06:14.466872 UTC] Start collecting samples
[2018-09-05 09:06:14.730961 UTC] Computing input variables for policy optimization
[2018-09-05 09:06:14.758695 UTC] Computing policy gradient
[2018-09-05 09:06:14.771155 UTC] Updating baseline
[2018-09-05 09:06:14.947231 UTC] Computing logging information
-------------------------------------
| Iteration            | 67         |
| SurrLoss             | -0.0029859 |
| Entropy              | 0.19314    |
| Perplexity           | 1.2131     |
| AveragePolicyProb[0] | 0.5046     |
| AveragePolicyProb[1] | 0.4954     |
| AverageReturn        | 200        |
| MinReturn            | 200        |
| MaxReturn            | 200        |
| StdReturn            | 0          |
| AverageEpisodeLength | 200        |
| MinEpisodeLength     | 200        |
| MaxEpisodeLength     | 200        |
| StdEpisodeLength     | 0          |
| TotalNEpisodes       | 821        |
| TotalNSamples        | 1.3426e+05 |
| ExplainedVariance    | 0.71119    |
-------------------------------------
[2018-09-05 09:06:14.999381 UTC] Saving snapshot
[2018-09-05 09:06:15.010520 UTC] Starting iteration 68
[2018-09-05 09:06:15.010724 UTC] Start collecting samples
[2018-09-05 09:06:15.294887 UTC] Computing input variables for policy optimization
[2018-09-05 09:06:15.315740 UTC] Computing policy gradient
[2018-09-05 09:06:15.326043 UTC] Updating baseline
[2018-09-05 09:06:15.454619 UTC] Computing logging information
-------------------------------------
| Iteration            | 68         |
| SurrLoss             | -0.0021754 |
| Entropy              | 0.1934     |
| Perplexity           | 1.2134     |
| AveragePolicyProb[0] | 0.49128    |
| AveragePolicyProb[1] | 0.50872    |
| AverageReturn        | 200        |
| MinReturn            | 200        |
| MaxReturn            | 200        |
| StdReturn            | 0          |
| AverageEpisodeLength | 200        |
| MinEpisodeLength     | 200        |
| MaxEpisodeLength     | 200        |
| StdEpisodeLength     | 0          |
| TotalNEpisodes       | 833        |
| TotalNSamples        | 1.3666e+05 |
| ExplainedVariance    | 0.72841    |
-------------------------------------
[2018-09-05 09:06:15.493097 UTC] Saving snapshot
[2018-09-05 09:06:15.500978 UTC] Starting iteration 69
[2018-09-05 09:06:15.501121 UTC] Start collecting samples
[2018-09-05 09:06:15.764659 UTC] Computing input variables for policy optimization
[2018-09-05 09:06:15.791489 UTC] Computing policy gradient
[2018-09-05 09:06:15.804609 UTC] Updating baseline
[2018-09-05 09:06:15.960898 UTC] Computing logging information
-------------------------------------
| Iteration            | 69         |
| SurrLoss             | 0.0014806  |
| Entropy              | 0.19029    |
| Perplexity           | 1.2096     |
| AveragePolicyProb[0] | 0.50035    |
| AveragePolicyProb[1] | 0.49965    |
| AverageReturn        | 200        |
| MinReturn            | 200        |
| MaxReturn            | 200        |
| StdReturn            | 0          |
| AverageEpisodeLength | 200        |
| MinEpisodeLength     | 200        |
| MaxEpisodeLength     | 200        |
| StdEpisodeLength     | 0          |
| TotalNEpisodes       | 842        |
| TotalNSamples        | 1.3846e+05 |
| ExplainedVariance    | 0.66559    |
-------------------------------------
[2018-09-05 09:06:16.003491 UTC] Saving snapshot
[2018-09-05 09:06:16.015590 UTC] Starting iteration 70
[2018-09-05 09:06:16.015741 UTC] Start collecting samples
[2018-09-05 09:06:16.284986 UTC] Computing input variables for policy optimization
[2018-09-05 09:06:16.312938 UTC] Computing policy gradient
[2018-09-05 09:06:16.325288 UTC] Updating baseline
[2018-09-05 09:06:16.464109 UTC] Computing logging information
--------------------------------------
| Iteration            | 70          |
| SurrLoss             | -0.00029992 |
| Entropy              | 0.18279     |
| Perplexity           | 1.2006      |
| AveragePolicyProb[0] | 0.50935     |
| AveragePolicyProb[1] | 0.49065     |
| AverageReturn        | 200         |
| MinReturn            | 200         |
| MaxReturn            | 200         |
| StdReturn            | 0           |
| AverageEpisodeLength | 200         |
| MinEpisodeLength     | 200         |
| MaxEpisodeLength     | 200         |
| StdEpisodeLength     | 0           |
| TotalNEpisodes       | 852         |
| TotalNSamples        | 1.4046e+05  |
| ExplainedVariance    | 0.60204     |
--------------------------------------
[2018-09-05 09:06:16.504404 UTC] Saving snapshot
[2018-09-05 09:06:16.512253 UTC] Starting iteration 71
[2018-09-05 09:06:16.512400 UTC] Start collecting samples
[2018-09-05 09:06:16.769197 UTC] Computing input variables for policy optimization
[2018-09-05 09:06:16.791079 UTC] Computing policy gradient
[2018-09-05 09:06:16.802041 UTC] Updating baseline
[2018-09-05 09:06:16.944327 UTC] Computing logging information
-------------------------------------
| Iteration            | 71         |
| SurrLoss             | -0.012997  |
| Entropy              | 0.1736     |
| Perplexity           | 1.1896     |
| AveragePolicyProb[0] | 0.49124    |
| AveragePolicyProb[1] | 0.50876    |
| AverageReturn        | 200        |
| MinReturn            | 200        |
| MaxReturn            | 200        |
| StdReturn            | 0          |
| AverageEpisodeLength | 200        |
| MinEpisodeLength     | 200        |
| MaxEpisodeLength     | 200        |
| StdEpisodeLength     | 0          |
| TotalNEpisodes       | 863        |
| TotalNSamples        | 1.4266e+05 |
| ExplainedVariance    | 0.52747    |
-------------------------------------
[2018-09-05 09:06:16.984015 UTC] Saving snapshot
[2018-09-05 09:06:16.991910 UTC] Starting iteration 72
[2018-09-05 09:06:16.992060 UTC] Start collecting samples
[2018-09-05 09:06:17.375905 UTC] Computing input variables for policy optimization
[2018-09-05 09:06:17.394919 UTC] Computing policy gradient
[2018-09-05 09:06:17.404551 UTC] Updating baseline
[2018-09-05 09:06:17.537067 UTC] Computing logging information
-------------------------------------
| Iteration            | 72         |
| SurrLoss             | 0.0028655  |
| Entropy              | 0.17668    |
| Perplexity           | 1.1932     |
| AveragePolicyProb[0] | 0.50402    |
| AveragePolicyProb[1] | 0.49598    |
| AverageReturn        | 200        |
| MinReturn            | 200        |
| MaxReturn            | 200        |
| StdReturn            | 0          |
| AverageEpisodeLength | 200        |
| MinEpisodeLength     | 200        |
| MaxEpisodeLength     | 200        |
| StdEpisodeLength     | 0          |
| TotalNEpisodes       | 870        |
| TotalNSamples        | 1.4406e+05 |
| ExplainedVariance    | 0.28878    |
-------------------------------------
[2018-09-05 09:06:17.575305 UTC] Saving snapshot
[2018-09-05 09:06:17.583584 UTC] Starting iteration 73
[2018-09-05 09:06:17.583729 UTC] Start collecting samples
[2018-09-05 09:06:17.872536 UTC] Computing input variables for policy optimization
[2018-09-05 09:06:17.895675 UTC] Computing policy gradient
[2018-09-05 09:06:17.905695 UTC] Updating baseline
[2018-09-05 09:06:18.036814 UTC] Computing logging information
-------------------------------------
| Iteration            | 73         |
| SurrLoss             | -0.0050901 |
| Entropy              | 0.18455    |
| Perplexity           | 1.2027     |
| AveragePolicyProb[0] | 0.4985     |
| AveragePolicyProb[1] | 0.5015     |
| AverageReturn        | 200        |
| MinReturn            | 200        |
| MaxReturn            | 200        |
| StdReturn            | 0          |
| AverageEpisodeLength | 200        |
| MinEpisodeLength     | 200        |
| MaxEpisodeLength     | 200        |
| StdEpisodeLength     | 0          |
| TotalNEpisodes       | 883        |
| TotalNSamples        | 1.4666e+05 |
| ExplainedVariance    | 0.14102    |
-------------------------------------
[2018-09-05 09:06:18.075025 UTC] Saving snapshot
[2018-09-05 09:06:18.083571 UTC] Starting iteration 74
[2018-09-05 09:06:18.083733 UTC] Start collecting samples
[2018-09-05 09:06:18.363355 UTC] Computing input variables for policy optimization
[2018-09-05 09:06:18.389436 UTC] Computing policy gradient
[2018-09-05 09:06:18.401623 UTC] Updating baseline
[2018-09-05 09:06:18.561171 UTC] Computing logging information
--------------------------------------
| Iteration            | 74          |
| SurrLoss             | -0.00037775 |
| Entropy              | 0.19603     |
| Perplexity           | 1.2166      |
| AveragePolicyProb[0] | 0.49897     |
| AveragePolicyProb[1] | 0.50103     |
| AverageReturn        | 200         |
| MinReturn            | 200         |
| MaxReturn            | 200         |
| StdReturn            | 0           |
| AverageEpisodeLength | 200         |
| MinEpisodeLength     | 200         |
| MaxEpisodeLength     | 200         |
| StdEpisodeLength     | 0           |
| TotalNEpisodes       | 891         |
| TotalNSamples        | 1.4826e+05  |
| ExplainedVariance    | 0.033635    |
--------------------------------------
[2018-09-05 09:06:18.600765 UTC] Saving snapshot
[2018-09-05 09:06:18.609543 UTC] Starting iteration 75
[2018-09-05 09:06:18.609694 UTC] Start collecting samples
[2018-09-05 09:06:18.895232 UTC] Computing input variables for policy optimization
[2018-09-05 09:06:18.915303 UTC] Computing policy gradient
[2018-09-05 09:06:18.925597 UTC] Updating baseline
[2018-09-05 09:06:19.058239 UTC] Computing logging information
--------------------------------------
| Iteration            | 75          |
| SurrLoss             | -0.00089418 |
| Entropy              | 0.18712     |
| Perplexity           | 1.2058      |
| AveragePolicyProb[0] | 0.49001     |
| AveragePolicyProb[1] | 0.50999     |
| AverageReturn        | 200         |
| MinReturn            | 200         |
| MaxReturn            | 200         |
| StdReturn            | 0           |
| AverageEpisodeLength | 200         |
| MinEpisodeLength     | 200         |
| MaxEpisodeLength     | 200         |
| StdEpisodeLength     | 0           |
| TotalNEpisodes       | 901         |
| TotalNSamples        | 1.5026e+05  |
| ExplainedVariance    | 0.014532    |
--------------------------------------
[2018-09-05 09:06:19.096247 UTC] Saving snapshot
[2018-09-05 09:06:19.104386 UTC] Starting iteration 76
[2018-09-05 09:06:19.104532 UTC] Start collecting samples
[2018-09-05 09:06:19.364333 UTC] Computing input variables for policy optimization
[2018-09-05 09:06:19.386287 UTC] Computing policy gradient
[2018-09-05 09:06:19.395788 UTC] Updating baseline
[2018-09-05 09:06:19.526763 UTC] Computing logging information
-------------------------------------
| Iteration            | 76         |
| SurrLoss             | 0.017091   |
| Entropy              | 0.19678    |
| Perplexity           | 1.2175     |
| AveragePolicyProb[0] | 0.50156    |
| AveragePolicyProb[1] | 0.49844    |
| AverageReturn        | 200        |
| MinReturn            | 200        |
| MaxReturn            | 200        |
| StdReturn            | 0          |
| AverageEpisodeLength | 200        |
| MinEpisodeLength     | 200        |
| MaxEpisodeLength     | 200        |
| StdEpisodeLength     | 0          |
| TotalNEpisodes       | 913        |
| TotalNSamples        | 1.5266e+05 |
| ExplainedVariance    | -0.07978   |
-------------------------------------
[2018-09-05 09:06:19.566402 UTC] Saving snapshot
[2018-09-05 09:06:19.574259 UTC] Starting iteration 77
[2018-09-05 09:06:19.574407 UTC] Start collecting samples
[2018-09-05 09:06:19.909576 UTC] Computing input variables for policy optimization
[2018-09-05 09:06:19.930251 UTC] Computing policy gradient
[2018-09-05 09:06:19.940175 UTC] Updating baseline
[2018-09-05 09:06:20.051678 UTC] Computing logging information
-------------------------------------
| Iteration            | 77         |
| SurrLoss             | -0.0029399 |
| Entropy              | 0.19634    |
| Perplexity           | 1.2169     |
| AveragePolicyProb[0] | 0.51045    |
| AveragePolicyProb[1] | 0.48955    |
| AverageReturn        | 200        |
| MinReturn            | 200        |
| MaxReturn            | 200        |
| StdReturn            | 0          |
| AverageEpisodeLength | 200        |
| MinEpisodeLength     | 200        |
| MaxEpisodeLength     | 200        |
| StdEpisodeLength     | 0          |
| TotalNEpisodes       | 922        |
| TotalNSamples        | 1.5446e+05 |
| ExplainedVariance    | 0.038327   |
-------------------------------------
[2018-09-05 09:06:20.090801 UTC] Saving snapshot
[2018-09-05 09:06:20.100421 UTC] Starting iteration 78
[2018-09-05 09:06:20.100669 UTC] Start collecting samples
[2018-09-05 09:06:20.362111 UTC] Computing input variables for policy optimization
[2018-09-05 09:06:20.382120 UTC] Computing policy gradient
[2018-09-05 09:06:20.391023 UTC] Updating baseline
[2018-09-05 09:06:20.510339 UTC] Computing logging information
-------------------------------------
| Iteration            | 78         |
| SurrLoss             | -0.0050033 |
| Entropy              | 0.20624    |
| Perplexity           | 1.2291     |
| AveragePolicyProb[0] | 0.48974    |
| AveragePolicyProb[1] | 0.51026    |
| AverageReturn        | 200        |
| MinReturn            | 200        |
| MaxReturn            | 200        |
| StdReturn            | 0          |
| AverageEpisodeLength | 200        |
| MinEpisodeLength     | 200        |
| MaxEpisodeLength     | 200        |
| StdEpisodeLength     | 0          |
| TotalNEpisodes       | 932        |
| TotalNSamples        | 1.5646e+05 |
| ExplainedVariance    | -0.094785  |
-------------------------------------
[2018-09-05 09:06:20.548012 UTC] Saving snapshot
[2018-09-05 09:06:20.555874 UTC] Starting iteration 79
[2018-09-05 09:06:20.556019 UTC] Start collecting samples
[2018-09-05 09:06:20.918059 UTC] Computing input variables for policy optimization
[2018-09-05 09:06:20.942940 UTC] Computing policy gradient
[2018-09-05 09:06:20.952070 UTC] Updating baseline
[2018-09-05 09:06:21.095318 UTC] Computing logging information
-------------------------------------
| Iteration            | 79         |
| SurrLoss             | 0.0087853  |
| Entropy              | 0.21338    |
| Perplexity           | 1.2379     |
| AveragePolicyProb[0] | 0.48859    |
| AveragePolicyProb[1] | 0.51141    |
| AverageReturn        | 200        |
| MinReturn            | 200        |
| MaxReturn            | 200        |
| StdReturn            | 0          |
| AverageEpisodeLength | 200        |
| MinEpisodeLength     | 200        |
| MaxEpisodeLength     | 200        |
| StdEpisodeLength     | 0          |
| TotalNEpisodes       | 943        |
| TotalNSamples        | 1.5866e+05 |
| ExplainedVariance    | 0.12603    |
-------------------------------------
[2018-09-05 09:06:21.138408 UTC] Saving snapshot
[2018-09-05 09:06:21.146760 UTC] Starting iteration 80
[2018-09-05 09:06:21.146909 UTC] Start collecting samples
[2018-09-05 09:06:21.411210 UTC] Computing input variables for policy optimization
[2018-09-05 09:06:21.429852 UTC] Computing policy gradient
[2018-09-05 09:06:21.439324 UTC] Updating baseline
[2018-09-05 09:06:21.573034 UTC] Computing logging information
-------------------------------------
| Iteration            | 80         |
| SurrLoss             | -0.011939  |
| Entropy              | 0.2234     |
| Perplexity           | 1.2503     |
| AveragePolicyProb[0] | 0.48659    |
| AveragePolicyProb[1] | 0.51341    |
| AverageReturn        | 200        |
| MinReturn            | 200        |
| MaxReturn            | 200        |
| StdReturn            | 0          |
| AverageEpisodeLength | 200        |
| MinEpisodeLength     | 200        |
| MaxEpisodeLength     | 200        |
| StdEpisodeLength     | 0          |
| TotalNEpisodes       | 950        |
| TotalNSamples        | 1.6006e+05 |
| ExplainedVariance    | 0.17452    |
-------------------------------------
[2018-09-05 09:06:21.614521 UTC] Saving snapshot
[2018-09-05 09:06:21.622778 UTC] Starting iteration 81
[2018-09-05 09:06:21.622934 UTC] Start collecting samples
[2018-09-05 09:06:21.909262 UTC] Computing input variables for policy optimization
[2018-09-05 09:06:21.931872 UTC] Computing policy gradient
[2018-09-05 09:06:21.942307 UTC] Updating baseline
[2018-09-05 09:06:22.078333 UTC] Computing logging information
-------------------------------------
| Iteration            | 81         |
| SurrLoss             | 0.028006   |
| Entropy              | 0.23692    |
| Perplexity           | 1.2673     |
| AveragePolicyProb[0] | 0.49962    |
| AveragePolicyProb[1] | 0.50038    |
| AverageReturn        | 200        |
| MinReturn            | 200        |
| MaxReturn            | 200        |
| StdReturn            | 0          |
| AverageEpisodeLength | 200        |
| MinEpisodeLength     | 200        |
| MaxEpisodeLength     | 200        |
| StdEpisodeLength     | 0          |
| TotalNEpisodes       | 963        |
| TotalNSamples        | 1.6266e+05 |
| ExplainedVariance    | 0.24235    |
-------------------------------------
[2018-09-05 09:06:22.117125 UTC] Saving snapshot
[2018-09-05 09:06:22.125108 UTC] Starting iteration 82
[2018-09-05 09:06:22.125261 UTC] Start collecting samples
[2018-09-05 09:06:22.387030 UTC] Computing input variables for policy optimization
[2018-09-05 09:06:22.407115 UTC] Computing policy gradient
[2018-09-05 09:06:22.417762 UTC] Updating baseline
[2018-09-05 09:06:22.561474 UTC] Computing logging information
-------------------------------------
| Iteration            | 82         |
| SurrLoss             | -0.021693  |
| Entropy              | 0.25174    |
| Perplexity           | 1.2863     |
| AveragePolicyProb[0] | 0.50387    |
| AveragePolicyProb[1] | 0.49614    |
| AverageReturn        | 200        |
| MinReturn            | 200        |
| MaxReturn            | 200        |
| StdReturn            | 0          |
| AverageEpisodeLength | 200        |
| MinEpisodeLength     | 200        |
| MaxEpisodeLength     | 200        |
| StdEpisodeLength     | 0          |
| TotalNEpisodes       | 971        |
| TotalNSamples        | 1.6426e+05 |
| ExplainedVariance    | 0.19781    |
-------------------------------------
[2018-09-05 09:06:22.600146 UTC] Saving snapshot
[2018-09-05 09:06:22.608323 UTC] Starting iteration 83
[2018-09-05 09:06:22.608549 UTC] Start collecting samples
[2018-09-05 09:06:22.935252 UTC] Computing input variables for policy optimization
[2018-09-05 09:06:22.956578 UTC] Computing policy gradient
[2018-09-05 09:06:22.965427 UTC] Updating baseline
[2018-09-05 09:06:23.105526 UTC] Computing logging information
-------------------------------------
| Iteration            | 83         |
| SurrLoss             | -0.0074405 |
| Entropy              | 0.25736    |
| Perplexity           | 1.2935     |
| AveragePolicyProb[0] | 0.49006    |
| AveragePolicyProb[1] | 0.50994    |
| AverageReturn        | 200        |
| MinReturn            | 200        |
| MaxReturn            | 200        |
| StdReturn            | 0          |
| AverageEpisodeLength | 200        |
| MinEpisodeLength     | 200        |
| MaxEpisodeLength     | 200        |
| StdEpisodeLength     | 0          |
| TotalNEpisodes       | 981        |
| TotalNSamples        | 1.6626e+05 |
| ExplainedVariance    | 0.33543    |
-------------------------------------
[2018-09-05 09:06:23.147049 UTC] Saving snapshot
[2018-09-05 09:06:23.155110 UTC] Starting iteration 84
[2018-09-05 09:06:23.155264 UTC] Start collecting samples
[2018-09-05 09:06:23.409866 UTC] Computing input variables for policy optimization
[2018-09-05 09:06:23.432515 UTC] Computing policy gradient
[2018-09-05 09:06:23.442650 UTC] Updating baseline
[2018-09-05 09:06:23.572473 UTC] Computing logging information
-------------------------------------
| Iteration            | 84         |
| SurrLoss             | -0.026725  |
| Entropy              | 0.28399    |
| Perplexity           | 1.3284     |
| AveragePolicyProb[0] | 0.50191    |
| AveragePolicyProb[1] | 0.49809    |
| AverageReturn        | 200        |
| MinReturn            | 200        |
| MaxReturn            | 200        |
| StdReturn            | 0          |
| AverageEpisodeLength | 200        |
| MinEpisodeLength     | 200        |
| MaxEpisodeLength     | 200        |
| StdEpisodeLength     | 0          |
| TotalNEpisodes       | 993        |
| TotalNSamples        | 1.6866e+05 |
| ExplainedVariance    | 0.39697    |
-------------------------------------
[2018-09-05 09:06:23.612461 UTC] Saving snapshot
[2018-09-05 09:06:23.620446 UTC] Starting iteration 85
[2018-09-05 09:06:23.620597 UTC] Start collecting samples
[2018-09-05 09:06:23.898912 UTC] Computing input variables for policy optimization
[2018-09-05 09:06:23.920864 UTC] Computing policy gradient
[2018-09-05 09:06:23.930776 UTC] Updating baseline
[2018-09-05 09:06:24.045763 UTC] Computing logging information
-------------------------------------
| Iteration            | 85         |
| SurrLoss             | 0.0026482  |
| Entropy              | 0.30332    |
| Perplexity           | 1.3543     |
| AveragePolicyProb[0] | 0.51059    |
| AveragePolicyProb[1] | 0.48941    |
| AverageReturn        | 196.22     |
| MinReturn            | 10         |
| MaxReturn            | 200        |
| StdReturn            | 26.46      |
| AverageEpisodeLength | 196.22     |
| MinEpisodeLength     | 10         |
| MaxEpisodeLength     | 200        |
| StdEpisodeLength     | 26.46      |
| TotalNEpisodes       | 1004       |
| TotalNSamples        | 1.7049e+05 |
| ExplainedVariance    | 0.36633    |
-------------------------------------
[2018-09-05 09:06:24.086619 UTC] Saving snapshot
[2018-09-05 09:06:24.095121 UTC] Starting iteration 86
[2018-09-05 09:06:24.095272 UTC] Start collecting samples
[2018-09-05 09:06:24.434386 UTC] Computing input variables for policy optimization
[2018-09-05 09:06:24.463885 UTC] Computing policy gradient
[2018-09-05 09:06:24.473044 UTC] Updating baseline
[2018-09-05 09:06:24.606116 UTC] Computing logging information
-------------------------------------
| Iteration            | 86         |
| SurrLoss             | 0.032412   |
| Entropy              | 0.28485    |
| Perplexity           | 1.3296     |
| AveragePolicyProb[0] | 0.50236    |
| AveragePolicyProb[1] | 0.49764    |
| AverageReturn        | 172.26     |
| MinReturn            | 10         |
| MaxReturn            | 200        |
| StdReturn            | 66.255     |
| AverageEpisodeLength | 172.26     |
| MinEpisodeLength     | 10         |
| MaxEpisodeLength     | 200        |
| StdEpisodeLength     | 66.255     |
| TotalNEpisodes       | 1027       |
| TotalNSamples        | 1.7269e+05 |
| ExplainedVariance    | 0.25304    |
-------------------------------------
[2018-09-05 09:06:24.646958 UTC] Saving snapshot
[2018-09-05 09:06:24.654916 UTC] Starting iteration 87
[2018-09-05 09:06:24.655101 UTC] Start collecting samples
[2018-09-05 09:06:24.924140 UTC] Computing input variables for policy optimization
[2018-09-05 09:06:24.946255 UTC] Computing policy gradient
[2018-09-05 09:06:24.955613 UTC] Updating baseline
[2018-09-05 09:06:25.070164 UTC] Computing logging information
-------------------------------------
| Iteration            | 87         |
| SurrLoss             | 0.05402    |
| Entropy              | 0.29015    |
| Perplexity           | 1.3366     |
| AveragePolicyProb[0] | 0.4847     |
| AveragePolicyProb[1] | 0.5153     |
| AverageReturn        | 169.09     |
| MinReturn            | 10         |
| MaxReturn            | 200        |
| StdReturn            | 68.746     |
| AverageEpisodeLength | 169.09     |
| MinEpisodeLength     | 10         |
| MaxEpisodeLength     | 200        |
| StdEpisodeLength     | 68.746     |
| TotalNEpisodes       | 1038       |
| TotalNSamples        | 1.7457e+05 |
| ExplainedVariance    | 0.44295    |
-------------------------------------
[2018-09-05 09:06:25.109626 UTC] Saving snapshot
[2018-09-05 09:06:25.118125 UTC] Starting iteration 88
[2018-09-05 09:06:25.118278 UTC] Start collecting samples
[2018-09-05 09:06:25.381528 UTC] Computing input variables for policy optimization
[2018-09-05 09:06:25.400824 UTC] Computing policy gradient
[2018-09-05 09:06:25.409557 UTC] Updating baseline
[2018-09-05 09:06:25.538192 UTC] Computing logging information
-------------------------------------
| Iteration            | 88         |
| SurrLoss             | 0.033326   |
| Entropy              | 0.30468    |
| Perplexity           | 1.3562     |
| AveragePolicyProb[0] | 0.49173    |
| AveragePolicyProb[1] | 0.50827    |
| AverageReturn        | 169.09     |
| MinReturn            | 10         |
| MaxReturn            | 200        |
| StdReturn            | 68.746     |
| AverageEpisodeLength | 169.09     |
| MinEpisodeLength     | 10         |
| MaxEpisodeLength     | 200        |
| StdEpisodeLength     | 68.746     |
| TotalNEpisodes       | 1046       |
| TotalNSamples        | 1.7617e+05 |
| ExplainedVariance    | 0.73845    |
-------------------------------------
[2018-09-05 09:06:25.578691 UTC] Saving snapshot
[2018-09-05 09:06:25.586758 UTC] Starting iteration 89
[2018-09-05 09:06:25.586915 UTC] Start collecting samples
[2018-09-05 09:06:25.857400 UTC] Computing input variables for policy optimization
[2018-09-05 09:06:25.886519 UTC] Computing policy gradient
[2018-09-05 09:06:25.898616 UTC] Updating baseline
[2018-09-05 09:06:26.020598 UTC] Computing logging information
-------------------------------------
| Iteration            | 89         |
| SurrLoss             | -0.0051773 |
| Entropy              | 0.29478    |
| Perplexity           | 1.3428     |
| AveragePolicyProb[0] | 0.4992     |
| AveragePolicyProb[1] | 0.5008     |
| AverageReturn        | 169.09     |
| MinReturn            | 10         |
| MaxReturn            | 200        |
| StdReturn            | 68.746     |
| AverageEpisodeLength | 169.09     |
| MinEpisodeLength     | 10         |
| MaxEpisodeLength     | 200        |
| StdEpisodeLength     | 68.746     |
| TotalNEpisodes       | 1057       |
| TotalNSamples        | 1.7837e+05 |
| ExplainedVariance    | 0.90192    |
-------------------------------------
[2018-09-05 09:06:26.061396 UTC] Saving snapshot
[2018-09-05 09:06:26.069509 UTC] Starting iteration 90
[2018-09-05 09:06:26.069665 UTC] Start collecting samples
[2018-09-05 09:06:26.345885 UTC] Computing input variables for policy optimization
[2018-09-05 09:06:26.367585 UTC] Computing policy gradient
[2018-09-05 09:06:26.376929 UTC] Updating baseline
[2018-09-05 09:06:26.527176 UTC] Computing logging information
-------------------------------------
| Iteration            | 90         |
| SurrLoss             | -0.021649  |
| Entropy              | 0.26926    |
| Perplexity           | 1.309      |
| AveragePolicyProb[0] | 0.48849    |
| AveragePolicyProb[1] | 0.51151    |
| AverageReturn        | 169.09     |
| MinReturn            | 10         |
| MaxReturn            | 200        |
| StdReturn            | 68.746     |
| AverageEpisodeLength | 169.09     |
| MinEpisodeLength     | 10         |
| MaxEpisodeLength     | 200        |
| StdEpisodeLength     | 68.746     |
| TotalNEpisodes       | 1067       |
| TotalNSamples        | 1.8037e+05 |
| ExplainedVariance    | 0.93324    |
-------------------------------------
[2018-09-05 09:06:26.568007 UTC] Saving snapshot
[2018-09-05 09:06:26.575935 UTC] Starting iteration 91
[2018-09-05 09:06:26.576083 UTC] Start collecting samples
[2018-09-05 09:06:26.859950 UTC] Computing input variables for policy optimization
[2018-09-05 09:06:26.880725 UTC] Computing policy gradient
[2018-09-05 09:06:26.889555 UTC] Updating baseline
[2018-09-05 09:06:27.019074 UTC] Computing logging information
-------------------------------------
| Iteration            | 91         |
| SurrLoss             | -0.023266  |
| Entropy              | 0.24924    |
| Perplexity           | 1.283      |
| AveragePolicyProb[0] | 0.50223    |
| AveragePolicyProb[1] | 0.49777    |
| AverageReturn        | 169.09     |
| MinReturn            | 10         |
| MaxReturn            | 200        |
| StdReturn            | 68.746     |
| AverageEpisodeLength | 169.09     |
| MinEpisodeLength     | 10         |
| MaxEpisodeLength     | 200        |
| StdEpisodeLength     | 68.746     |
| TotalNEpisodes       | 1077       |
| TotalNSamples        | 1.8237e+05 |
| ExplainedVariance    | 0.86598    |
-------------------------------------
[2018-09-05 09:06:27.060575 UTC] Saving snapshot
[2018-09-05 09:06:27.069400 UTC] Starting iteration 92
[2018-09-05 09:06:27.069550 UTC] Start collecting samples
[2018-09-05 09:06:27.333551 UTC] Computing input variables for policy optimization
[2018-09-05 09:06:27.353899 UTC] Computing policy gradient
[2018-09-05 09:06:27.363042 UTC] Updating baseline
[2018-09-05 09:06:27.492552 UTC] Computing logging information
-------------------------------------
| Iteration            | 92         |
| SurrLoss             | -0.020982  |
| Entropy              | 0.24094    |
| Perplexity           | 1.2724     |
| AveragePolicyProb[0] | 0.51239    |
| AveragePolicyProb[1] | 0.48761    |
| AverageReturn        | 169.09     |
| MinReturn            | 10         |
| MaxReturn            | 200        |
| StdReturn            | 68.746     |
| AverageEpisodeLength | 169.09     |
| MinEpisodeLength     | 10         |
| MaxEpisodeLength     | 200        |
| StdEpisodeLength     | 68.746     |
| TotalNEpisodes       | 1088       |
| TotalNSamples        | 1.8457e+05 |
| ExplainedVariance    | 0.81406    |
-------------------------------------
[2018-09-05 09:06:27.532169 UTC] Saving snapshot
[2018-09-05 09:06:27.540636 UTC] Starting iteration 93
[2018-09-05 09:06:27.540782 UTC] Start collecting samples
[2018-09-05 09:06:27.810823 UTC] Computing input variables for policy optimization
[2018-09-05 09:06:27.832139 UTC] Computing policy gradient
[2018-09-05 09:06:27.842047 UTC] Updating baseline
[2018-09-05 09:06:27.970747 UTC] Computing logging information
-------------------------------------
| Iteration            | 93         |
| SurrLoss             | -0.023907  |
| Entropy              | 0.22738    |
| Perplexity           | 1.2553     |
| AveragePolicyProb[0] | 0.50329    |
| AveragePolicyProb[1] | 0.49671    |
| AverageReturn        | 172.87     |
| MinReturn            | 10         |
| MaxReturn            | 200        |
| StdReturn            | 65.046     |
| AverageEpisodeLength | 172.87     |
| MinEpisodeLength     | 10         |
| MaxEpisodeLength     | 200        |
| StdEpisodeLength     | 65.046     |
| TotalNEpisodes       | 1099       |
| TotalNSamples        | 1.8677e+05 |
| ExplainedVariance    | 0.82815    |
-------------------------------------
[2018-09-05 09:06:28.010629 UTC] Saving snapshot
[2018-09-05 09:06:28.018562 UTC] Starting iteration 94
[2018-09-05 09:06:28.018720 UTC] Start collecting samples
[2018-09-05 09:06:28.259557 UTC] Computing input variables for policy optimization
[2018-09-05 09:06:28.278731 UTC] Computing policy gradient
[2018-09-05 09:06:28.288348 UTC] Updating baseline
[2018-09-05 09:06:28.427240 UTC] Computing logging information
-------------------------------------
| Iteration            | 94         |
| SurrLoss             | -0.0041934 |
| Entropy              | 0.21302    |
| Perplexity           | 1.2374     |
| AveragePolicyProb[0] | 0.49625    |
| AveragePolicyProb[1] | 0.50375    |
| AverageReturn        | 174.75     |
| MinReturn            | 10         |
| MaxReturn            | 200        |
| StdReturn            | 63.056     |
| AverageEpisodeLength | 174.75     |
| MinEpisodeLength     | 10         |
| MaxEpisodeLength     | 200        |
| StdEpisodeLength     | 63.056     |
| TotalNEpisodes       | 1107       |
| TotalNSamples        | 1.8837e+05 |
| ExplainedVariance    | 0.91593    |
-------------------------------------
[2018-09-05 09:06:28.468557 UTC] Saving snapshot
[2018-09-05 09:06:28.477069 UTC] Starting iteration 95
[2018-09-05 09:06:28.477272 UTC] Start collecting samples
[2018-09-05 09:06:28.741943 UTC] Computing input variables for policy optimization
[2018-09-05 09:06:28.763535 UTC] Computing policy gradient
[2018-09-05 09:06:28.772590 UTC] Updating baseline
[2018-09-05 09:06:28.906201 UTC] Computing logging information
-------------------------------------
| Iteration            | 95         |
| SurrLoss             | -0.0014338 |
| Entropy              | 0.20793    |
| Perplexity           | 1.2311     |
| AveragePolicyProb[0] | 0.49688    |
| AveragePolicyProb[1] | 0.50312    |
| AverageReturn        | 186.07     |
| MinReturn            | 10         |
| MaxReturn            | 200        |
| StdReturn            | 47.789     |
| AverageEpisodeLength | 186.07     |
| MinEpisodeLength     | 10         |
| MaxEpisodeLength     | 200        |
| StdEpisodeLength     | 47.789     |
| TotalNEpisodes       | 1118       |
| TotalNSamples        | 1.9057e+05 |
| ExplainedVariance    | 0.89325    |
-------------------------------------
[2018-09-05 09:06:28.945696 UTC] Saving snapshot
[2018-09-05 09:06:28.953642 UTC] Starting iteration 96
[2018-09-05 09:06:28.953792 UTC] Start collecting samples
[2018-09-05 09:06:29.206519 UTC] Computing input variables for policy optimization
[2018-09-05 09:06:29.227082 UTC] Computing policy gradient
[2018-09-05 09:06:29.235712 UTC] Updating baseline
[2018-09-05 09:06:29.364666 UTC] Computing logging information
-------------------------------------
| Iteration            | 96         |
| SurrLoss             | -0.011626  |
| Entropy              | 0.21627    |
| Perplexity           | 1.2414     |
| AveragePolicyProb[0] | 0.49347    |
| AveragePolicyProb[1] | 0.50653    |
| AverageReturn        | 194.93     |
| MinReturn            | 10         |
| MaxReturn            | 200        |
| StdReturn            | 29.285     |
| AverageEpisodeLength | 194.93     |
| MinEpisodeLength     | 10         |
| MaxEpisodeLength     | 200        |
| StdEpisodeLength     | 29.285     |
| TotalNEpisodes       | 1126       |
| TotalNSamples        | 1.9217e+05 |
| ExplainedVariance    | 0.93465    |
-------------------------------------
[2018-09-05 09:06:29.404934 UTC] Saving snapshot
[2018-09-05 09:06:29.414637 UTC] Starting iteration 97
[2018-09-05 09:06:29.414836 UTC] Start collecting samples
[2018-09-05 09:06:29.679129 UTC] Computing input variables for policy optimization
[2018-09-05 09:06:29.700452 UTC] Computing policy gradient
[2018-09-05 09:06:29.710800 UTC] Updating baseline
[2018-09-05 09:06:29.843012 UTC] Computing logging information
-------------------------------------
| Iteration            | 97         |
| SurrLoss             | 0.0049881  |
| Entropy              | 0.20186    |
| Perplexity           | 1.2237     |
| AveragePolicyProb[0] | 0.49777    |
| AveragePolicyProb[1] | 0.50223    |
| AverageReturn        | 200        |
| MinReturn            | 200        |
| MaxReturn            | 200        |
| StdReturn            | 0          |
| AverageEpisodeLength | 200        |
| MinEpisodeLength     | 200        |
| MaxEpisodeLength     | 200        |
| StdEpisodeLength     | 0          |
| TotalNEpisodes       | 1137       |
| TotalNSamples        | 1.9437e+05 |
| ExplainedVariance    | 0.85297    |
-------------------------------------
[2018-09-05 09:06:29.889731 UTC] Saving snapshot
[2018-09-05 09:06:29.897809 UTC] Starting iteration 98
[2018-09-05 09:06:29.897963 UTC] Start collecting samples
[2018-09-05 09:06:30.153795 UTC] Computing input variables for policy optimization
[2018-09-05 09:06:30.175236 UTC] Computing policy gradient
[2018-09-05 09:06:30.184648 UTC] Updating baseline
[2018-09-05 09:06:30.315808 UTC] Computing logging information
-------------------------------------
| Iteration            | 98         |
| SurrLoss             | -0.025973  |
| Entropy              | 0.19454    |
| Perplexity           | 1.2147     |
| AveragePolicyProb[0] | 0.49205    |
| AveragePolicyProb[1] | 0.50795    |
| AverageReturn        | 200        |
| MinReturn            | 200        |
| MaxReturn            | 200        |
| StdReturn            | 0          |
| AverageEpisodeLength | 200        |
| MinEpisodeLength     | 200        |
| MaxEpisodeLength     | 200        |
| StdEpisodeLength     | 0          |
| TotalNEpisodes       | 1147       |
| TotalNSamples        | 1.9637e+05 |
| ExplainedVariance    | 0.94656    |
-------------------------------------
[2018-09-05 09:06:30.356300 UTC] Saving snapshot
[2018-09-05 09:06:30.364266 UTC] Starting iteration 99
[2018-09-05 09:06:30.364411 UTC] Start collecting samples
[2018-09-05 09:06:30.624440 UTC] Computing input variables for policy optimization
[2018-09-05 09:06:30.644812 UTC] Computing policy gradient
[2018-09-05 09:06:30.654666 UTC] Updating baseline
[2018-09-05 09:06:30.791547 UTC] Computing logging information
-------------------------------------
| Iteration            | 99         |
| SurrLoss             | -0.00478   |
| Entropy              | 0.19804    |
| Perplexity           | 1.219      |
| AveragePolicyProb[0] | 0.50472    |
| AveragePolicyProb[1] | 0.49528    |
| AverageReturn        | 200        |
| MinReturn            | 200        |
| MaxReturn            | 200        |
| StdReturn            | 0          |
| AverageEpisodeLength | 200        |
| MinEpisodeLength     | 200        |
| MaxEpisodeLength     | 200        |
| StdEpisodeLength     | 0          |
| TotalNEpisodes       | 1157       |
| TotalNSamples        | 1.9837e+05 |
| ExplainedVariance    | 0.93436    |
-------------------------------------
[2018-09-05 09:06:30.835236 UTC] Saving snapshot
